{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e09810",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37372,
     "status": "ok",
     "timestamp": 1766323345400,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "18e09810",
    "outputId": "dcf69d52-cd60-413e-f2e7-076c7ce15602"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "google_drive_path = ''\n",
    "os.chdir(google_drive_path)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "!ls\n",
    "\n",
    "!python -m pip install trimesh\n",
    "!python -m pip install rtree\n",
    "!python -m pip install cripser==0.0.15 #scikit-image trimesh plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "from tqdm import trange\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# To allow imports from parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\"))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f094e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.Point_Sampling.point_sampler import Point_Sampler\n",
    "from Test_Cases.Jet_Engine_Bracket.JEB_Master_object import JEB_Master_object\n",
    "from Test_Cases.Bridge_around_object.BRIDGE_Master_object import BRIDGE_Master_Object\n",
    "from Functions.Training.Properties import Properties \n",
    "from Functions.Plotting_functions.training_curves.JEB_PINN_curves import *\n",
    "from Functions.Computations.L2_error import compute_L2_errors_3d \n",
    "from Functions.logging.JEB_PINN_logging import save_metrics_csv_3d \n",
    "from Functions.Plotting_functions.JEB_PINN_results import plot_results \n",
    "from Functions.utils import * \n",
    "\n",
    "from Functions.Data_preprocessing_functions.interpolate_from_PointCloud import (\n",
    "    interpolate_from_point_cloud,\n",
    ")\n",
    "from Models.GINN_Models.GINN import GINN\n",
    "\n",
    "from File_Paths.file_paths import (\n",
    "    mesh_path,\n",
    "    point_cloud_path,\n",
    "    data_path,\n",
    ")\n",
    "\n",
    "BRIDGE = BRIDGE_Master_Object(Normalize=True, Symmetry=False)\n",
    "BRIDGE.create_interfaces()\n",
    "\n",
    "JEB = JEB_Master_object(Normalize=True, Symmetry=False)\n",
    "JEB.create_interfaces()\n",
    "material_properties = Properties(test_case=BRIDGE)\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fc425",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128343,
     "status": "ok",
     "timestamp": 1766323473786,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "b39fc425",
    "outputId": "0cd5ed14-cda1-428b-87ae-b42b542c9278"
   },
   "outputs": [],
   "source": [
    "\n",
    "s = float(JEB.domain_scaling_factor)                \n",
    "center_mm = np.asarray(JEB.domain_center, dtype=float)\n",
    "\n",
    "\n",
    "# Load and normalize 411 geometry \n",
    "simple_geometry_mesh_filename = \"411.obj\"\n",
    "simple_geometry_PC_filename   = \"411_PC_data.csv\"\n",
    "\n",
    "simple_geometry_mesh = trimesh.load(os.path.join(mesh_path, simple_geometry_mesh_filename))\n",
    "simple_geometry_mesh.fix_normals()\n",
    "simple_geometry_mesh.fill_holes()\n",
    "\n",
    "simple_geometry_point_cloud = pd.read_csv(\n",
    "    os.path.join(point_cloud_path, simple_geometry_PC_filename)\n",
    ").to_numpy()\n",
    "\n",
    "# Normalize with JEB transform: \n",
    "simple_geometry_mesh.apply_translation(-center_mm)\n",
    "simple_geometry_mesh.apply_scale(s)\n",
    "\n",
    "simple_geometry_point_cloud[:, :3] = (simple_geometry_point_cloud[:, :3] - center_mm[np.newaxis, :]) * s\n",
    "simple_geometry_point_cloud[:, 3] *= s  \n",
    "\n",
    "solid_volume_hat = float(simple_geometry_mesh.volume)\n",
    "solid_volume_mm3 = solid_volume_hat / (s**3)\n",
    "\n",
    "\n",
    "\n",
    "# Load FEM reference from Excel \n",
    "def load_JEB_FEM_reference():\n",
    "    filename = \"JEB_411_validation_FEM_mine.xlsx\"\n",
    "    file_path = Path(data_path) / filename\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"[WARN] FEM file not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    def norm(name: str) -> str:\n",
    "        return \"\".join(name.lower().split())\n",
    "\n",
    "    cols_norm = {norm(c): c for c in df.columns}\n",
    "\n",
    "    def find_col(possible_names):\n",
    "        for name in possible_names:\n",
    "            key = norm(name)\n",
    "            if key in cols_norm:\n",
    "                return cols_norm[key]\n",
    "        raise KeyError(\n",
    "            f\"None of {possible_names} found.\\nAvailable columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    col_x = find_col([\"X Location (mm)\"])\n",
    "    col_y = find_col([\"Y Location (mm)\"])\n",
    "    col_z = find_col([\"Z Location (mm)\"])\n",
    "\n",
    "    coords_mm = df[[col_x, col_y, col_z]].to_numpy(dtype=float)\n",
    "\n",
    "    # normalized coordinates consistent with JEB\n",
    "    coords_scaled = (coords_mm - center_mm) * s\n",
    "\n",
    "    col_ux = find_col([\"X Disp (mm)\", \"X disp (mm)\", \"X Displacement (mm)\"])\n",
    "    col_uy = find_col([\"Y Disp (mm)\", \"Y disp (mm)\", \"Y Displacement (mm)\"])\n",
    "    col_uz = find_col([\"Z Disp (mm)\", \"Z disp (mm)\", \"Z Displacement (mm)\"])\n",
    "\n",
    "    x_disp = df[col_ux].to_numpy(dtype=float)\n",
    "    y_disp = df[col_uy].to_numpy(dtype=float)\n",
    "    z_disp = df[col_uz].to_numpy(dtype=float)\n",
    "\n",
    "    col_svm = find_col(\n",
    "        [\n",
    "            \"Equivalent (von-Mises) Stress (MPa)\",\n",
    "            \"Equivalent von-Mises Stress (MPa)\",\n",
    "            \"Equivalent (von Mises) Stress (MPa)\",\n",
    "        ]\n",
    "    )\n",
    "    sigma_vm = df[col_svm].to_numpy(dtype=float)\n",
    "\n",
    "    fem_ref = {\n",
    "        \"coords_mm\": coords_mm,\n",
    "        \"coords_scaled\": coords_scaled,\n",
    "        \"x_disp\": x_disp,\n",
    "        \"y_disp\": y_disp,\n",
    "        \"z_disp\": z_disp,\n",
    "        \"sigma_vm\": sigma_vm,\n",
    "    }\n",
    "\n",
    "    print(\"Loaded FEM:\", file_path)\n",
    "    print(\"FEM nodes:\", coords_mm.shape[0])\n",
    "\n",
    "    return fem_ref\n",
    "\n",
    "\n",
    "\n",
    "def fem_inside_mask_from_sdf(fem_ref, sdf_inside_is_negative=True):\n",
    "    if fem_ref is None:\n",
    "        return None\n",
    "\n",
    "    coords_hat = fem_ref[\"coords_scaled\"]\n",
    "    sdf_hat = interpolate_from_point_cloud(simple_geometry_point_cloud, coords_hat, quantity=\"SDF\")\n",
    "    sdf_hat = np.asarray(sdf_hat).reshape(-1)\n",
    "\n",
    "    if sdf_inside_is_negative:\n",
    "        inside = sdf_hat < 0.0\n",
    "    else:\n",
    "        inside = sdf_hat > 0.0\n",
    "\n",
    "    print(f\"inside fraction by SDF: {inside.mean():.4f}\")\n",
    "    fem_ref[\"inside_mask\"] = inside\n",
    "    fem_ref[\"sdf_hat\"] = sdf_hat\n",
    "    return fem_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8986be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(torch.nn.Module):\n",
    "    def __init__(self, hparams_model, hparams_feature_expansion, mollifier_alpha):\n",
    "        super().__init__()\n",
    "        self.mollifier_alpha = mollifier_alpha\n",
    "        self.model = GINN(\n",
    "            JEB,\n",
    "            feature_expansion=hparams_feature_expansion,\n",
    "            Model_hyperparameters=hparams_model,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def enforce_dirichlet_BC(alpha, u, coords):\n",
    "        device_local = u.device if isinstance(coords, torch.Tensor) else None\n",
    "\n",
    "        radius = JEB.bolt_interface_radius\n",
    "        depth = JEB.bolt_interface_depth  \n",
    "\n",
    "        centroid1 = JEB.bolt1_centroid\n",
    "        centroid2 = JEB.bolt2_centroid\n",
    "        centroid3 = JEB.bolt3_centroid\n",
    "        centroid4 = JEB.bolt4_centroid\n",
    "\n",
    "        centroids = torch.tensor(\n",
    "            [\n",
    "                centroid1[0:2],\n",
    "                centroid2[0:2],\n",
    "                centroid3[0:2],\n",
    "                centroid4[0:2],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device_local,\n",
    "        )\n",
    "\n",
    "        x = coords[:, 0]\n",
    "        y = coords[:, 1]\n",
    "        xy_coords = torch.stack([x, y], dim=1)\n",
    "\n",
    "        multiplier_total = torch.ones(coords.shape[0], device=device_local)\n",
    "\n",
    "        for centroid in centroids:\n",
    "            dist = torch.norm(xy_coords - centroid, dim=1)\n",
    "\n",
    "            inside_mask = dist <= radius\n",
    "            outside_mask = dist > radius\n",
    "\n",
    "            d_inside = torch.abs(dist[inside_mask] - radius)\n",
    "            d_outside = torch.abs(radius - dist[outside_mask])\n",
    "\n",
    "            m_inside = torch.tanh(alpha * d_inside)\n",
    "            m_outside = torch.tanh(alpha * d_outside)\n",
    "\n",
    "            multiplier = torch.ones_like(dist)\n",
    "            multiplier[inside_mask] = m_inside\n",
    "            multiplier[outside_mask] = m_outside\n",
    "\n",
    "            multiplier_total *= multiplier\n",
    "\n",
    "        return u * multiplier_total.unsqueeze(1)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        u = self.model(coords)\n",
    "        return self.enforce_dirichlet_BC(self.mollifier_alpha, u, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Loss(Properties):\n",
    "    def __init__(self, u_model, v_model, w_model, training_hparams):\n",
    "        super().__init__(test_case=JEB)\n",
    "\n",
    "        self.u_model = u_model.to(device)\n",
    "        self.v_model = v_model.to(device)\n",
    "        self.w_model = w_model.to(device)\n",
    "\n",
    "        self.num_neumann_pts = int(training_hparams[\"num_neumann_points\"])\n",
    "        self.p = float(training_hparams[\"density_exponent\"])\n",
    "        self.rho_min = float(training_hparams[\"rho_min\"])\n",
    "\n",
    "        self.V_solid_mm3 = float(solid_volume_mm3)\n",
    "\n",
    "    def ritz_loss(self, coords_hat, rho):\n",
    "        neumann_hat = self.interfaces.sample_points_on_neumann_boundary(\n",
    "            self.num_neumann_pts, \"vertical\", \"torch_tensor\"\n",
    "        ).to(device)\n",
    "        neumann_hat.requires_grad_(True)\n",
    "\n",
    "        R_hat = float(JEB.pinn_interface_radius)\n",
    "        width_hat = float(JEB.pinn_interface_width)\n",
    "        arc_area_hat = 2.0 * math.pi * R_hat * width_hat\n",
    "        arc_area_mm2 = arc_area_hat / (s**2)\n",
    "\n",
    "        ds_mm2 = arc_area_mm2 / neumann_hat.shape[0]\n",
    "\n",
    "        traction_z = self.force_vector[2] / (2*arc_area_mm2)\n",
    "\n",
    "\n",
    "        w_neu = self.w_model(neumann_hat).squeeze(-1)\n",
    "\n",
    "        external_energy = torch.sum(traction_z * w_neu * ds_mm2)\n",
    "        x = coords_hat.detach().clone().requires_grad_(True)\n",
    "\n",
    "        u = self.u_model(x)  \n",
    "        v = self.v_model(x)  \n",
    "        w = self.w_model(x)  \n",
    "\n",
    "        gu_hat = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "        gv_hat = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True, retain_graph=True)[0]\n",
    "        gw_hat = torch.autograd.grad(w, x, grad_outputs=torch.ones_like(w), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "        gu = s * gu_hat\n",
    "        gv = s * gv_hat\n",
    "        gw = s * gw_hat\n",
    "\n",
    "        e11 = gu[:, 0]\n",
    "        e22 = gv[:, 1]\n",
    "        e33 = gw[:, 2]\n",
    "        e12 = 0.5 * (gu[:, 1] + gv[:, 0])\n",
    "        e13 = 0.5 * (gu[:, 2] + gw[:, 0])\n",
    "        e23 = 0.5 * (gv[:, 2] + gw[:, 1])\n",
    "        tr = e11 + e22 + e33\n",
    "\n",
    "        rho = rho.view(-1).clamp(0.0, 1.0)\n",
    "        rho_p = rho.pow(self.p) if self.p != 0 else torch.ones_like(rho)\n",
    "        if self.rho_min > 0.0:\n",
    "            rho_p = self.rho_min + (1.0 - self.rho_min) * rho_p\n",
    "\n",
    "        lam = self.lame_lambda * rho_p\n",
    "        mu  = self.lame_mu * rho_p\n",
    "\n",
    "        s11 = 2 * mu * e11 + lam * tr\n",
    "        s22 = 2 * mu * e22 + lam * tr\n",
    "        s33 = 2 * mu * e33 + lam * tr\n",
    "        s12 = 2 * mu * e12\n",
    "        s13 = 2 * mu * e13\n",
    "        s23 = 2 * mu * e23\n",
    "\n",
    "        psi = 0.5 * (\n",
    "            s11 * e11 + s22 * e22 + s33 * e33\n",
    "            + 2 * s12 * e12 + 2 * s13 * e13 + 2 * s23 * e23\n",
    "        )  \n",
    "\n",
    "        internal_energy = self.V_solid_mm3 * psi.mean()  \n",
    "\n",
    "        return internal_energy - external_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FixedGeometryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Samples points in JEB.domain (normalized coords).\n",
    "    Interpolates SDF from normalized point cloud.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_points: int, training_hparams: dict):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        point_sampler = Point_Sampler(\n",
    "            JEB.domain,\n",
    "            JEB.interfaces,\n",
    "            num_points_domain=num_points,\n",
    "            num_points_interface=0,\n",
    "        )\n",
    "        pts_hat = next(point_sampler)  # torch CPU\n",
    "\n",
    "        pts_np = pts_hat.detach().cpu().numpy()\n",
    "        sdf_np = interpolate_from_point_cloud(simple_geometry_point_cloud, pts_np, quantity=\"SDF\")\n",
    "        sdf_hat = torch.tensor(sdf_np, dtype=torch.float32, device=self.device).view(-1)\n",
    "\n",
    "        inside_is_negative = bool(training_hparams[\"sdf_inside_is_negative\"])\n",
    "        if inside_is_negative:\n",
    "            rho = (sdf_hat < 0.0).float()\n",
    "        else:\n",
    "            rho = (sdf_hat > 0.0).float()\n",
    "\n",
    "        if bool(training_hparams[\"solid_only_sampling\"]):\n",
    "            mask = rho > 0.5\n",
    "            self.points = pts_hat.to(self.device)[mask]\n",
    "            self.density = rho[mask]\n",
    "        else:\n",
    "            self.points = pts_hat.to(self.device)\n",
    "            self.density = rho\n",
    "\n",
    "        with torch.no_grad():\n",
    "            frac_solid = float((rho > 0.5).float().mean().item())\n",
    "            self.stats = {\n",
    "                \"solid_frac_in_domain\": frac_solid,\n",
    "                \"points_used\": int(self.points.shape[0]),\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.points[idx], self.density[idx]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams_model = {\n",
    "    \"SIREN_hparams\": {\n",
    "        \"Model_type\": \"SIREN\",\n",
    "        \"layers\": [180, 180, 180, 180, 180],\n",
    "        \"dimensionality\": 3,\n",
    "        \"w0_initial\": 60,\n",
    "        \"w0\": 10,\n",
    "        \"skip_connection\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "hparams_feature_expansion = {\n",
    "    \"Feature Type\": \"None\",\n",
    "    \"Num Frequencies\": 3,\n",
    "    \"Max Frequency\": 100,\n",
    "}\n",
    "\n",
    "training_hparams = {\n",
    "    \"total_points\": 900_000,\n",
    "    \"batch_size\": 300_000,\n",
    "    \"num_epochs\": 5002,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"gamma\": 0.75,\n",
    "    \"num_neumann_points\": 70_000,\n",
    "    \"mollifier_alpha\": 1.0,\n",
    "    \"density_exponent\": 10,  \n",
    "    \"rho_min\": 0.0,         \n",
    "    \"density_mode\": \"binary_sdf\",\n",
    "    \"sdf_inside_is_negative\": True, \n",
    "    \"solid_only_sampling\": True,\n",
    "    \"plot_interval\": 200,\n",
    "    \"save_path\": \"./JEB_PINN_val_final\",\n",
    "    \"seed\": 43,\n",
    "\n",
    "    \"resume\": False,                 # set False to force a fresh run\n",
    "    \"save_every_epochs\": 200,        # checkpoint \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae3d6c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766323473790,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "cfae3d6c"
   },
   "outputs": [],
   "source": [
    "\n",
    "JEB_FEM_ref = load_JEB_FEM_reference() \n",
    "SDF_INSIDE_IS_NEGATIVE = True\n",
    "JEB_FEM_ref = fem_inside_mask_from_sdf(JEB_FEM_ref, sdf_inside_is_negative=SDF_INSIDE_IS_NEGATIVE)\n",
    "\n",
    "# Training loop \n",
    "def train_pinn_for_fixed_geometry():\n",
    "    set_random_seed(training_hparams[\"seed\"])\n",
    "\n",
    "    save_dir = training_hparams[\"save_path\"]\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- checkpoint paths ---\n",
    "    latest_ckpt_path = os.path.join(save_dir, \"checkpoint_latest.pth\")\n",
    "    best_model_path  = os.path.join(save_dir, \"best_model_optionA_fixed.pth\")\n",
    "\n",
    "    # --- models ---\n",
    "    u_model = PINN(\n",
    "        hparams_model[\"SIREN_hparams\"],\n",
    "        hparams_feature_expansion,\n",
    "        training_hparams[\"mollifier_alpha\"],\n",
    "    ).to(device)\n",
    "\n",
    "    v_model = PINN(\n",
    "        hparams_model[\"SIREN_hparams\"],\n",
    "        hparams_feature_expansion,\n",
    "        training_hparams[\"mollifier_alpha\"],\n",
    "    ).to(device)\n",
    "\n",
    "    w_model = PINN(\n",
    "        hparams_model[\"SIREN_hparams\"],\n",
    "        hparams_feature_expansion,\n",
    "        training_hparams[\"mollifier_alpha\"],\n",
    "    ).to(device)\n",
    "\n",
    "    params = list(u_model.parameters()) + list(v_model.parameters()) + list(w_model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=params,\n",
    "        lr=training_hparams[\"learning_rate\"],\n",
    "        weight_decay=training_hparams[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=int(training_hparams[\"num_epochs\"] / 4),\n",
    "        gamma=training_hparams[\"gamma\"],\n",
    "    )\n",
    "\n",
    "    loss_class_disp = PINN_Loss(u_model, v_model, w_model, training_hparams)\n",
    "\n",
    "    # --- resume training logic --- \n",
    "    start_epoch = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    history = {\"epoch\": [], \"loss\": [], \"L2_u\": [], \"L2_v\": [], \"L2_w\": [], \"L2_s\": []}\n",
    "\n",
    "    if training_hparams.get(\"resume\", True) and os.path.exists(latest_ckpt_path):\n",
    "        print(f\"[RESUME] Loading checkpoint: {latest_ckpt_path}\")\n",
    "        last_epoch, best_loss, history = load_checkpoint(\n",
    "            latest_ckpt_path,\n",
    "            u_model, v_model, w_model,\n",
    "            optimizer, scheduler,\n",
    "            map_location=device\n",
    "        )\n",
    "        start_epoch = last_epoch + 1\n",
    "        print(f\"[RESUME] Resuming from epoch {start_epoch} | best_loss={best_loss:.6e}\")\n",
    "    else:\n",
    "        print(\"[RESUME] Starting fresh training run.\")\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    save_every = int(training_hparams.get(\"save_every_epochs\", 10))\n",
    "    plot_interval = int(training_hparams[\"plot_interval\"])\n",
    "\n",
    "    try:\n",
    "        for epoch in trange(start_epoch, training_hparams[\"num_epochs\"]):\n",
    "            dataset = FixedGeometryDataset(training_hparams[\"total_points\"], training_hparams)\n",
    "\n",
    "            if (epoch % plot_interval) == 0:\n",
    "                print(\"[dataset]\", dataset.stats)\n",
    "\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=training_hparams[\"batch_size\"],\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for coords_batch, density_batch in dataloader:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss_ritz = loss_class_disp.ritz_loss(coords_batch, density_batch)\n",
    "                loss_ritz.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss_ritz.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            epoch_loss /= max(1, len(dataloader))\n",
    "\n",
    "            # --- metrics ---\n",
    "            L2_u, L2_v, L2_w, L2_s = compute_L2_errors_3d(\n",
    "                u_model, v_model, w_model, material_properties, JEB_FEM_ref, JEB\n",
    "            )\n",
    "\n",
    "            history[\"epoch\"].append(epoch)\n",
    "            history[\"loss\"].append(epoch_loss)\n",
    "            history[\"L2_u\"].append(0.0 if L2_u is None else L2_u)\n",
    "            history[\"L2_v\"].append(0.0 if L2_v is None else L2_v)\n",
    "            history[\"L2_w\"].append(0.0 if L2_w is None else L2_w)\n",
    "            history[\"L2_s\"].append(0.0 if L2_s is None else L2_s)\n",
    "\n",
    "            # --- save best model (same behavior as before) ---\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                atomic_torch_save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"loss\": best_loss,\n",
    "                        \"u_model_state_dict\": u_model.state_dict(),\n",
    "                        \"v_model_state_dict\": v_model.state_dict(),\n",
    "                        \"w_model_state_dict\": w_model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    },\n",
    "                    best_model_path, \n",
    "                )\n",
    "\n",
    "            if (epoch % save_every) == 0 or (epoch % plot_interval) == 0:\n",
    "                save_checkpoint(\n",
    "                    latest_ckpt_path,\n",
    "                    epoch=epoch,\n",
    "                    u_model=u_model,\n",
    "                    v_model=v_model,\n",
    "                    w_model=w_model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    best_loss=best_loss,\n",
    "                    history=history,\n",
    "                )\n",
    "\n",
    "            if (epoch % plot_interval) == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, loss={epoch_loss:.6e}, \"\n",
    "                    f\"lr={scheduler.optimizer.param_groups[0]['lr']:.3e}, \"\n",
    "                    f\"best_loss={best_loss:.6e}, \"\n",
    "                    f\"L2_u={history['L2_u'][-1]:.3e}, \"\n",
    "                    f\"L2_v={history['L2_v'][-1]:.3e}, \"\n",
    "                    f\"L2_w={history['L2_w'][-1]:.3e}, \"\n",
    "                    f\"L2_s={history['L2_s'][-1]:.3e}\"\n",
    "                )\n",
    "\n",
    "                plot_results(material_properties, u_model, v_model, w_model, JEB_FEM_ref, save_dir, epoch,JEB)\n",
    "\n",
    "                fname_curves_lin = os.path.join(save_dir, f\"training_curves_3d_{epoch:06d}.png\")\n",
    "                fname_curves_log = os.path.join(save_dir, f\"training_curves_3d_log_{epoch:06d}.png\")\n",
    "                save_training_curves_3d(history, fname_curves_lin)\n",
    "                save_training_curves_log_3d(history, fname_curves_log)\n",
    "                save_metrics_csv_3d(history, save_dir)\n",
    "\n",
    "        # final save\n",
    "        save_checkpoint(\n",
    "            latest_ckpt_path,\n",
    "            epoch=training_hparams[\"num_epochs\"] - 1,\n",
    "            u_model=u_model,\n",
    "            v_model=v_model,\n",
    "            w_model=w_model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            best_loss=best_loss,\n",
    "            history=history,\n",
    "        )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[INTERRUPT] Caught KeyboardInterrupt — saving latest checkpoint before exit.\")\n",
    "        safe_epoch = history[\"epoch\"][-1] if len(history[\"epoch\"]) else (start_epoch - 1)\n",
    "        save_checkpoint(\n",
    "            latest_ckpt_path,\n",
    "            epoch=safe_epoch,\n",
    "            u_model=u_model,\n",
    "            v_model=v_model,\n",
    "            w_model=w_model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            best_loss=best_loss,\n",
    "            history=history,\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    print(\"Best loss:\", best_loss)\n",
    "    print(\"Best model path:\", best_model_path)\n",
    "    print(\"Latest checkpoint:\", latest_ckpt_path)\n",
    "\n",
    "    return best_model_path, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d7ef3",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1766323473815,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "7a5d7ef3"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_hparams.update({\n",
    "    \"resume\": True,                 # set False to force a fresh run\n",
    "    \"save_every_epochs\": 200,        # checkpoint cadence (in epochs)\n",
    "})\n",
    "\n",
    "best_model_path, history = train_pinn_for_fixed_geometry()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
