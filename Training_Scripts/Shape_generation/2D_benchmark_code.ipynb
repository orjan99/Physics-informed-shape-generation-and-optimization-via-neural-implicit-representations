{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba67a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5682,
     "status": "ok",
     "timestamp": 1770668087624,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "8dba67a4",
    "outputId": "e6b262ce-2a68-4a86-bb0a-86513c05b3f5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "google_drive_path = ''\n",
    "os.chdir(google_drive_path)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "!ls\n",
    "\n",
    "!python -m pip install cripser==0.0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302d64",
   "metadata": {
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1770668089658,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "ad302d64"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from torchsummary import summary\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tabulate import tabulate\n",
    "import cripser\n",
    "\n",
    "# To allow me to import the functions from other folders from the parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2cf9c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1764,
     "status": "ok",
     "timestamp": 1770668091426,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "15f2cf9c"
   },
   "outputs": [],
   "source": [
    "from Functions.utils import *\n",
    "from Functions.Point_Sampling.point_sampler import *\n",
    "from Functions.logging.BRIDGE_logging import *\n",
    "from Functions.Computations.PH2D import *\n",
    "from Functions.Point_Sampling.BS2D import *\n",
    "from Functions.Training.enforce_Const_2D import *\n",
    "from Functions.Plotting_functions.training_curves.BRIDGE_curves import *\n",
    "from Functions.Plotting_functions.BRIDGE_results import *\n",
    "from Functions.Training.ALM import *\n",
    "from Functions.Computations.eval2D import *\n",
    "\n",
    "\n",
    "from File_Paths.file_paths import interfaces_path, mesh_path\n",
    "from Functions.Point_Sampling.point_sampler import Point_Sampler\n",
    "from Models.GINN_Models.GINN import GINN\n",
    "\n",
    "\n",
    "from Test_Cases.Bridge_around_object.BRIDGE_Master_object import BRIDGE_Master_Object\n",
    "\n",
    "BRIDGE = BRIDGE_Master_Object(Normalize=True,Symmetry=False)\n",
    "BRIDGE.create_interfaces()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebed21c",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770668091432,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "aebed21c"
   },
   "outputs": [],
   "source": [
    "class SDF_GINN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams_model,\n",
    "                 hparams_feature_expansion):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model = GINN(BRIDGE,\n",
    "                          feature_expansion = hparams_feature_expansion ,\n",
    "                          Model_hyperparameters = hparams_model)\n",
    "\n",
    "    def forward(self,coords):\n",
    "        SDF = self.model(coords)\n",
    "        return SDF\n",
    "\n",
    "\n",
    "class density_GINN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams_model,\n",
    "                 hparams_feature_expansion,\n",
    "                 density_alpha,\n",
    "                 volume_ratio):\n",
    "\n",
    "        super().__init__()\n",
    "        self.volume_ratio = volume_ratio\n",
    "        self.density_alpha = density_alpha\n",
    "        self.model = GINN(BRIDGE,\n",
    "                          feature_expansion = hparams_feature_expansion ,\n",
    "                          Model_hyperparameters = hparams_model)\n",
    "\n",
    "    def forward(self,coords):\n",
    "        SDF = self.model(coords)\n",
    "        v = torch.as_tensor(self.volume_ratio, dtype=SDF.dtype, device=SDF.device)\n",
    "        offset = torch.log(v / (1.0 - v))\n",
    "        rho = torch.sigmoid(self.density_alpha * SDF + offset)\n",
    "        return rho.clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "class Geometry_Net(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                hparams_model,\n",
    "                hparams_feature_expansion,\n",
    "                density_alpha,\n",
    "                volume_ratio):\n",
    "\n",
    "        super().__init__()\n",
    "        self.volume_ratio = volume_ratio\n",
    "        self.density_alpha = density_alpha\n",
    "        self.model = GINN(BRIDGE,\n",
    "                        feature_expansion = hparams_feature_expansion ,\n",
    "                        Model_hyperparameters = hparams_model)\n",
    "\n",
    "    def forward(self,coords):\n",
    "        SDF = self.model(coords)\n",
    "        v = torch.as_tensor(self.volume_ratio, dtype=SDF.dtype, device=SDF.device)\n",
    "        offset = torch.log(v / (1.0 - v))\n",
    "        rho = torch.sigmoid(-self.density_alpha * SDF + offset)\n",
    "        #rho = torch.sigmoid(-10 * SDF)\n",
    "        rho = rho.clamp(1e-6, 1.0)\n",
    "        return rho, SDF\n",
    "\n",
    "\n",
    "class PINN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams_model,\n",
    "                 hparams_feature_expansion,\n",
    "                 mollifier_alpha):\n",
    "\n",
    "        super().__init__()\n",
    "        self.mollifier_alpha = mollifier_alpha\n",
    "\n",
    "        self.model = GINN(BRIDGE,\n",
    "                          feature_expansion = hparams_feature_expansion ,\n",
    "                          Model_hyperparameters = hparams_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def enforce_dirichlet_BC(alpha,u,coords):\n",
    "\n",
    "        edge_vertices = BRIDGE.edge_vertices\n",
    "\n",
    "        device = coords.device if isinstance(coords, torch.Tensor) else None\n",
    "        if isinstance(edge_vertices, torch.Tensor):\n",
    "            x_left  = edge_vertices[0, 0].item()\n",
    "            x_right = edge_vertices[2, 0].item()\n",
    "        else:\n",
    "            x_left  = edge_vertices[0, 0]\n",
    "            x_right = edge_vertices[2, 0]\n",
    "\n",
    "        distances_left = torch.abs(coords[:, 0] - x_left)\n",
    "        distances_right = torch.abs(coords[:, 0] - x_right)\n",
    "        multiplier_left = torch.tanh(alpha*distances_left)\n",
    "        multiplier_right = torch.tanh(alpha*distances_right)\n",
    "        multiplier = multiplier_left * multiplier_right\n",
    "        if device is not None:\n",
    "                multiplier = multiplier.to(device)\n",
    "\n",
    "        return u*multiplier.unsqueeze(1)\n",
    "\n",
    "    def forward(self,coords):\n",
    "        u = self.model(coords)\n",
    "        return self.enforce_dirichlet_BC(self.mollifier_alpha,u,coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce41b99",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770668091435,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "bce41b99"
   },
   "outputs": [],
   "source": [
    "class GINN_losses(Properties):\n",
    "    def __init__(self,\n",
    "                 GINN_model,\n",
    "                 test_case,\n",
    "                 GINN_hparams,\n",
    "                 PH,\n",
    "                 boundary_sampler,\n",
    "                 enforce_density):\n",
    "\n",
    "        super().__init__(test_case=test_case)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.GINN_model = GINN_model.to(self.device)\n",
    "        self.GINN_hparams = GINN_hparams\n",
    "        self.boundary_sampler = boundary_sampler\n",
    "        self.test_case = test_case\n",
    "        self.enforce_density = enforce_density\n",
    "        self.dim = test_case.dim # 2D or 3D\n",
    "        self.envelope_extension_factor = self.GINN_hparams['envelope_extension_factor']\n",
    "        self.num_points_connectivity_loss = self.GINN_hparams['num_points_connectivity_loss']\n",
    "        self.num_points_interface_loss = self.GINN_hparams['num_points_interface_loss']\n",
    "        self.num_points_normals_loss = self.GINN_hparams['num_points_normals_loss']\n",
    "        self.num_points_envelope_loss = self.GINN_hparams['num_points_envelope_loss']\n",
    "        self.clip_max_value = self.GINN_hparams['clip_max_value']\n",
    "        self.clip_min_value = self.GINN_hparams['clip_min_value']\n",
    "        self.max_curv = self.GINN_hparams['max_curv']\n",
    "        self.curv_start_epoch = self.GINN_hparams['curv_start_epoch']\n",
    "        self.PH = PH\n",
    "\n",
    "\n",
    "    def surface_normal_loss(self):\n",
    "\n",
    "        #1. Import prescribed surface normals for the test case and the points on the relevant surfaces\n",
    "        normals = self.test_case.interfaces.get_all_prescribed_surface_normals(num_points=self.num_points_normals_loss,\n",
    "                                                                               type='torch_tensor',\n",
    "                                                                               include_all=True)\n",
    "\n",
    "        points = normals['points'].to(self.device)\n",
    "\n",
    "\n",
    "        neumann_points          = points[normals['neumann_idx']]\n",
    "        dirichlet_points        = points[normals['dirichlet_idx']]\n",
    "\n",
    "        input = torch.vstack([neumann_points, dirichlet_points]).to(self.device)\n",
    "        input = input.requires_grad_(True)\n",
    "        target_surface_normals = torch.vstack([normals['neumann_normals'],\n",
    "                                               normals['dirichlet_normals']]).to(self.device)\n",
    "\n",
    "        #2. Predict the surface normals using the GINN model\n",
    "        SDF_values = self.GINN_model(input).view(-1)\n",
    "        predicted_surface_normals = torch.autograd.grad(inputs=input,\n",
    "                                            outputs=SDF_values,\n",
    "                                            grad_outputs=torch.ones_like(SDF_values),\n",
    "                                            create_graph=True,\n",
    "                                            only_inputs=True)[0]\n",
    "\n",
    "        # 3. Normalize the predicted surface normals\n",
    "        predicted_surface_normals = F.normalize(predicted_surface_normals, p=2 ,dim=1)\n",
    "\n",
    "        #3. Compute the loss as the mean squared error between predicted and target surface normals\n",
    "        loss = F.mse_loss(predicted_surface_normals, target_surface_normals)\n",
    "\n",
    "        #4. Return the loss value\n",
    "        return loss\n",
    "\n",
    "    def prohibited_region_loss(self, coords):\n",
    "\n",
    "        # 1. Identify the points inside the interface regions\n",
    "        inside_points, inside_mask = self.test_case.interfaces.is_inside_prohibited_region(coords)\n",
    "\n",
    "        # 2. Identify the SDF values at the inside points\n",
    "        SDF = self.GINN_model(coords).squeeze()\n",
    "        SDF_inside = SDF[inside_mask]\n",
    "        SDF_inside = SDF_inside.view(-1)\n",
    "\n",
    "        # Build a 1-D boolean mask of the violations\n",
    "        violation_mask = (SDF_inside < 0)\n",
    "\n",
    "        # 3. Filter out the points that have SDF > 0 --> Points are considered to be outside the surface\n",
    "        point_violations = inside_points[violation_mask]\n",
    "        SDF_violations = SDF_inside[violation_mask]\n",
    "\n",
    "        if SDF_violations.numel() > 0:\n",
    "            loss = torch.square(SDF_violations).sum()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=SDF.device, dtype=torch.float32,requires_grad=True)  # No points violate the interface constraint\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def prescribed_thickness_loss(self, coords):\n",
    "\n",
    "        # 1. Identify the points inside the prescribed thickness region\n",
    "        inside_points, inside_mask = self.test_case.interfaces.is_inside_interface_thickness(coords)\n",
    "\n",
    "        # 2. Collect the SDF values at the inside points\n",
    "        SDF = self.GINN_model(coords).squeeze()\n",
    "        SDF_inside = SDF[inside_mask]\n",
    "        SDF_inside = SDF_inside.view(-1)\n",
    "\n",
    "        # Build a 1-D boolean mask of the violations\n",
    "        violation_mask = (SDF_inside > 0)\n",
    "\n",
    "        # 3. Filter out point that have SDF > 0 --> Point are considered to be outside the surface\n",
    "        point_violations = inside_points[violation_mask]\n",
    "        point_violations.requires_grad_(True)\n",
    "        SDF_violations = SDF_inside[violation_mask]\n",
    "\n",
    "        # 4. Calculate the loss\n",
    "        if SDF_violations.numel() > 0:\n",
    "            prescribed_thickness_loss = torch.square(SDF_violations).sum()\n",
    "        else:\n",
    "            prescribed_thickness_loss = torch.tensor(0.0, device=SDF.device, dtype=torch.float32,requires_grad=True)  # No points violate the prescribed thickness constraint\n",
    "\n",
    "        return prescribed_thickness_loss\n",
    "\n",
    "    def interface_loss(self):\n",
    "\n",
    "        #1. Sample points in the interface region\n",
    "        coords = self.test_case.interfaces.sample_points_from_all_interfaces(self.num_points_interface_loss,\n",
    "                                                                            random_seed = None,\n",
    "                                                                            output_type = 'torch_tensor')\n",
    "\n",
    "        #2. Calulate the SDF values at the sampled points\n",
    "        input = coords.detach().to(self.device)\n",
    "        sdf_values = self.GINN_model(input).view(-1,1)\n",
    "        target_sdf_values = torch.zeros_like(sdf_values)\n",
    "\n",
    "        #3. Calculate the interface loss\n",
    "        interface_loss = F.mse_loss(sdf_values, target_sdf_values)\n",
    "        return interface_loss\n",
    "\n",
    "    def eikonal_loss(self,coords):\n",
    "\n",
    "        #1. Compute the gradient of the SDF with respect to the input coordinates\n",
    "        coords.requires_grad_(True)\n",
    "        SDF = self.GINN_model(coords).squeeze()\n",
    "        SDF_grad = torch.autograd.grad(outputs=SDF, inputs=coords, grad_outputs=torch.ones_like(SDF), create_graph=True)[0]\n",
    "\n",
    "        #2. Compute the norm of the gradient - this is the magnitude of the gradient vector\n",
    "        SDF_grad_norm = torch.norm(SDF_grad, dim=1)\n",
    "\n",
    "        #3. Compute the Eikonal loss\n",
    "        eikonal_loss = torch.mean((SDF_grad_norm - 1) ** 2)\n",
    "\n",
    "        return eikonal_loss\n",
    "\n",
    "\n",
    "    def design_envelope_loss(self):\n",
    "\n",
    "        # 1. Extract the domain from the test case\n",
    "        design_envelope = self.test_case.domain\n",
    "        x_min_domain = design_envelope[0]\n",
    "        x_max_domain = design_envelope[1]\n",
    "        y_min_domain = design_envelope[2]\n",
    "        y_max_domain = design_envelope[3]\n",
    "\n",
    "\n",
    "        # 2. Define the extended domain region\n",
    "        extension_factor = self.GINN_hparams['envelope_extension_factor']\n",
    "        x_min_extended = x_min_domain - extension_factor * (x_max_domain - x_min_domain)\n",
    "        x_max_extended = x_max_domain + extension_factor * (x_max_domain - x_min_domain)\n",
    "        y_min_extended = y_min_domain - extension_factor * (y_max_domain - y_min_domain)\n",
    "        y_max_extended = y_max_domain + extension_factor * (y_max_domain - y_min_domain)\n",
    "\n",
    "        extended_domain = np.array([x_min_extended, x_max_extended, y_min_extended, y_max_extended])\n",
    "\n",
    "\n",
    "        # 3. Sample points in the extended domain\n",
    "        point_sampler = Point_Sampler(extended_domain,\n",
    "                                      num_points_domain=self.num_points_envelope_loss)\n",
    "        points = next(point_sampler)\n",
    "        points = points.to(self.device)\n",
    "        points.requires_grad_(True)\n",
    "\n",
    "        # 4. Create a mask to remove the points that lie within the design envelope\n",
    "        condition1 = torch.logical_and(points[:, 0] >= x_min_domain, points[:, 0] <= x_max_domain)\n",
    "        condition2 = torch.logical_and(points[:, 1] >= y_min_domain, points[:, 1] <= y_max_domain)\n",
    "        mask = torch.logical_and(condition1, condition2)\n",
    "\n",
    "        # 5. Filter the points that lie outside the design envelope\n",
    "        points_outside_envelope = points[~mask]\n",
    "\n",
    "        # Return an error if no points are sampled outside the design envelope\n",
    "        if points_outside_envelope.shape[0] == 0:\n",
    "            raise ValueError(\"No points sampled outside the design envelope. Please increase the number of points or the extension factor.\")\n",
    "\n",
    "        # 6. Pass the points through the GINN model to compute the SDF values\n",
    "        sdf_values = self.GINN_model(points_outside_envelope).view(-1)  # Ensure the SDF values are a 1-D tensor\n",
    "\n",
    "\n",
    "        # 7. Filter the SDF values that are less or equal to zero\n",
    "        violation_mask = (sdf_values <= 0)        # shape: (M,)\n",
    "        SDF_constraint_violations = sdf_values[violation_mask]\n",
    "        if SDF_constraint_violations.numel() > 0:\n",
    "            points_constraint_violations = points_outside_envelope[violation_mask]\n",
    "            points_constraint_violations.requires_grad_(True)\n",
    "            envelope_loss = torch.square(SDF_constraint_violations).sum()\n",
    "        else:\n",
    "            envelope_loss = torch.tensor(0.0, device=sdf_values.device, dtype=torch.float32,requires_grad=True )    # No points violate the design envelope constraint\n",
    "        return envelope_loss\n",
    "\n",
    "\n",
    "    def design_envelope_loss_from_density(self,\n",
    "                                          density_model: torch.nn.Module,\n",
    "                                          iso_level: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Density-based analogue of design_envelope_loss.\n",
    "        \"\"\"\n",
    "        rho_iso = iso_level\n",
    "\n",
    "        # 1. Extract envelope bounds\n",
    "        design_envelope = self.test_case.domain\n",
    "        x_min_domain = design_envelope[0]\n",
    "        x_max_domain = design_envelope[1]\n",
    "        y_min_domain = design_envelope[2]\n",
    "        y_max_domain = design_envelope[3]\n",
    "\n",
    "        # 2. Extended domain\n",
    "        extension_factor = self.GINN_hparams['envelope_extension_factor']\n",
    "        x_min_extended = x_min_domain - extension_factor * (x_max_domain - x_min_domain)\n",
    "        x_max_extended = x_max_domain + extension_factor * (x_max_domain - x_min_domain)\n",
    "        y_min_extended = y_min_domain - extension_factor * (y_max_domain - y_min_domain)\n",
    "        y_max_extended = y_max_domain + extension_factor * (y_max_domain - y_min_domain)\n",
    "\n",
    "        extended_domain = np.array([x_min_extended, x_max_extended,\n",
    "                                    y_min_extended, y_max_extended])\n",
    "\n",
    "        # 3. Sample points in extended domain\n",
    "        point_sampler = Point_Sampler(\n",
    "            extended_domain,\n",
    "            num_points_domain=self.num_points_envelope_loss\n",
    "        )\n",
    "        points = next(point_sampler).to(self.device)\n",
    "        points.requires_grad_(True)\n",
    "\n",
    "        # 4. Mask out points inside the design envelope\n",
    "        condition1 = torch.logical_and(points[:, 0] >= x_min_domain,\n",
    "                                       points[:, 0] <= x_max_domain)\n",
    "        condition2 = torch.logical_and(points[:, 1] >= y_min_domain,\n",
    "                                       points[:, 1] <= y_max_domain)\n",
    "        mask_inside = torch.logical_and(condition1, condition2)\n",
    "\n",
    "        points_outside_envelope = points[~mask_inside]\n",
    "\n",
    "        if points_outside_envelope.shape[0] == 0:\n",
    "            raise ValueError(\n",
    "                \"No points sampled outside the design envelope. \"\n",
    "                \"Please increase the number of points or the extension factor.\"\n",
    "            )\n",
    "\n",
    "        # 5. Evaluate density at outside points\n",
    "        rho_values = density_model(points_outside_envelope).view(-1)\n",
    "\n",
    "        # 6. Penalize material outside the envelope\n",
    "        violation_mask = (rho_values > rho_iso)\n",
    "        rho_viol = rho_values[violation_mask]\n",
    "\n",
    "        if rho_viol.numel() > 0:\n",
    "            envelope_loss = torch.square(rho_viol).sum()\n",
    "        else:\n",
    "            envelope_loss = torch.tensor(\n",
    "                0.0,\n",
    "                device=rho_values.device,\n",
    "                dtype=torch.float32,\n",
    "                requires_grad=True\n",
    "            )\n",
    "\n",
    "        return envelope_loss\n",
    "\n",
    "\n",
    "    def smoothness_loss(self,epoch) -> torch.Tensor:\n",
    "\n",
    "        if epoch < self.curv_start_epoch:\n",
    "             return torch.tensor(0.0, device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "        surface_points, weights = self.boundary_sampler.get_surface_pts()\n",
    "        if surface_points is None:\n",
    "            print('Returning Zero Loss - No Surface Points Found')\n",
    "            return torch.tensor(0.0, device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        if surface_points.numel() == 0:\n",
    "            print('Returning Zero Loss - No Surface Points Found')\n",
    "            return torch.tensor(0.0, device=device, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        pts = surface_points.to(device)\n",
    "        if pts.dim() != 2:\n",
    "            raise ValueError(f\"surface_points must be [B, D], got {pts.shape}\")\n",
    "\n",
    "        w = weights.to(device)\n",
    "        if w.dim() == 2 and w.size(-1) == 1:\n",
    "            w = w.squeeze(-1)\n",
    "        if w.dim() != 1 or w.shape[0] != pts.shape[0]:\n",
    "            raise ValueError(f\"weights must be [B] or [B,1] matching points. Got {weights.shape}\")\n",
    "\n",
    "        B, D = pts.shape\n",
    "\n",
    "        # First-order derivatives\n",
    "        pts = pts.clone().detach().requires_grad_(True)\n",
    "        sdf = self.GINN_model(pts).view(-1)\n",
    "        grad_outputs = torch.ones_like(sdf)\n",
    "\n",
    "        df_dx = torch.autograd.grad(\n",
    "            outputs=sdf,\n",
    "            inputs=pts,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]                                            # [B, D]\n",
    "\n",
    "        # --- Second-order derivatives (Hessian wrt x) ---\n",
    "        H_rows = []\n",
    "        for d in range(D):\n",
    "            g_comp = df_dx[:, d]                        # [B]\n",
    "            Hg = torch.autograd.grad(\n",
    "                outputs=g_comp,\n",
    "                inputs=pts,\n",
    "                grad_outputs=torch.ones_like(g_comp),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "                only_inputs=True,\n",
    "            )[0]                                        # [B, D]\n",
    "            H_rows.append(Hg.unsqueeze(1))              # [B, 1, D]\n",
    "        H = torch.cat(H_rows, dim=1)                    # [B, D, D]\n",
    "\n",
    "        # Gaussian curvature \n",
    "        grad_sq = (df_dx.square()).sum(dim=1)           # [B]\n",
    "        F4 = torch.clamp(grad_sq.square(), min=1.0e-15)\n",
    "\n",
    "        # Build augmented matrix [[H, df_dx], [df_dx^T, 0]] per batch \n",
    "        top = torch.cat([H, df_dx.unsqueeze(2)], dim=2)                     # [B, D, D+1]\n",
    "        bottom = torch.cat([df_dx.unsqueeze(1), torch.zeros(B, 1, 1, device=device, dtype=H.dtype)], dim=2)\n",
    "        aug = torch.cat([top, bottom], dim=1)                                # [B, D+1, D+1]\n",
    "\n",
    "        det_aug = torch.det(aug)                                             # [B]\n",
    "        gauss_curvatures = (-1.0) / F4 * det_aug                             # [B]\n",
    "\n",
    "        #  Mean curvature\n",
    "        FHFT = torch.einsum('bi,bij,bj->b', df_dx, H, df_dx)                 # [B]\n",
    "        trH = torch.einsum('bii->b', H)                                      # [B]\n",
    "        N = torch.clamp(grad_sq.sqrt(), min=1.0e-5)\n",
    "        mean_curvatures = -(FHFT - (N.pow(2) * trH)) / (2.0 * N.pow(3))      # [B]\n",
    "\n",
    "        #  E-strain & clipping\n",
    "        E_strain = (2.0 * mean_curvatures).pow(2) - 2.0 * gauss_curvatures   # [B]\n",
    "        E_strain = torch.clamp(E_strain, min=self.clip_min_value, max=self.clip_max_value)\n",
    "\n",
    "        # Weight & sum\n",
    "        E_strain = E_strain * w\n",
    "        total = E_strain.sum()                                               # scalar\n",
    "\n",
    "        # Hinge with max_curv\n",
    "        zero = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "        loss = torch.maximum(zero, total - float(self.max_curv))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def connectivity_loss(self) -> torch.Tensor:\n",
    "        loss = self.PH.connectedness_loss()\n",
    "        return loss\n",
    "\n",
    "    def connectivity_loss_from_density(self,\n",
    "                                       density_model: torch.nn.Module,\n",
    "                                       iso_level: float = 0.5) -> torch.Tensor:\n",
    "        return self.PH.connectedness_loss_from_density(density_model, iso_level)\n",
    "\n",
    "\n",
    "    def holes_loss(self):\n",
    "        loss = self.PH.holes_loss()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def holes_loss_from_density(self,\n",
    "                                density_model: torch.nn.Module,\n",
    "                                iso_level: float = 0.5) -> torch.Tensor:\n",
    "        return self.PH.holes_loss_from_density(density_model, iso_level)\n",
    "\n",
    "    def learn_original_geometry(self,coords):\n",
    "        \"Learn human generated geometry for validation experiments\"\n",
    "        true_SDF = BRIDGE.interfaces.calculate_SDF(coords).squeeze()\n",
    "        predicted_SDF = self.GINN_model(coords).squeeze()\n",
    "        loss = F.mse_loss(predicted_SDF,true_SDF)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1abd01",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1770668091437,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "9c1abd01"
   },
   "outputs": [],
   "source": [
    "class PINN_losses(Properties):\n",
    "    def __init__(self,\n",
    "                 u_model,\n",
    "                 v_model,\n",
    "                 GINN_model,\n",
    "                 n_opt_samples,\n",
    "                 training_hparams,\n",
    "                 enforce_density):\n",
    "        super().__init__(test_case=BRIDGE)\n",
    "        self.u_model = u_model.to(self.device)\n",
    "        self.v_model = v_model.to(self.device)\n",
    "        self.GINN_model = GINN_model.to(self.device)\n",
    "        self.n_opt_samples = n_opt_samples\n",
    "        self.enforce_density = enforce_density\n",
    "        self.dirichlet_pts = training_hparams['dirichlet_pts']\n",
    "        self.num_neumann_pts = training_hparams['num_neumann_points']\n",
    "        self.density_exponent = training_hparams['density_exponent']\n",
    "\n",
    "    def ritz_loss(self,x):\n",
    "\n",
    "      # External Work\n",
    "      neumann_pts = self.interfaces.sample_points_on_neumann_boundary(self.num_neumann_pts, 'vertical', 'torch_tensor').to(self.device)\n",
    "      neumann_pts.requires_grad_(True)\n",
    "      R = BRIDGE.interfaces.obstacle_radius\n",
    "      arc_length = np.pi * R\n",
    "      ds = arc_length / neumann_pts.shape[0]\n",
    "\n",
    "      # Apply constant vertical traction\n",
    "      traction_y = self.force_vector[1] / arc_length\n",
    "      prescribed_traction = torch.zeros_like(neumann_pts)\n",
    "      prescribed_traction[:, 1] = traction_y\n",
    "\n",
    "      u_neu = self.u_model(neumann_pts).squeeze(-1)\n",
    "      v_neu = self.v_model(neumann_pts).squeeze(-1)\n",
    "      displacements_neumann = torch.stack([u_neu, v_neu], dim=1)\n",
    "\n",
    "      work = torch.sum(prescribed_traction * displacements_neumann, dim=1)\n",
    "      external_energy = torch.sum(work * ds)\n",
    "\n",
    "      # Internal Strain Energy\n",
    "      densities = self.GINN_model(x)\n",
    "      densities, _ = self.enforce_density.apply(x,densities, None, self.n_opt_samples, BRIDGE.domain)\n",
    "\n",
    "      coords = x.detach().clone().requires_grad_(True).to(self.device)\n",
    "      u = self.u_model(coords)\n",
    "      v = self.v_model(coords)\n",
    "\n",
    "      grad_u = torch.autograd.grad(u, coords, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "      grad_v = torch.autograd.grad(v, coords, grad_outputs=torch.ones_like(v), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "      epsilon_11 = grad_u[:, 0]\n",
    "      epsilon_22 = grad_v[:, 1]\n",
    "      epsilon_12 = 0.5 * (grad_u[:, 1] + grad_v[:, 0])\n",
    "      trace_epsilon = epsilon_11 + epsilon_22\n",
    "\n",
    "      # Update lame parameters based on the SIMP Method\n",
    "      densities = densities.squeeze()\n",
    "      lame_lambda = self.lame_lambda * torch.ones_like(epsilon_11)\n",
    "      lame_lambda = densities.clamp(0.0, 1.0).pow(self.density_exponent)*lame_lambda\n",
    "\n",
    "      lame_mu = self.lame_mu * torch.ones_like(epsilon_11)\n",
    "      lame_mu = densities.clamp(0.0, 1.0).pow(self.density_exponent)*lame_mu\n",
    "\n",
    "      sigma_11 = 2 * (lame_mu * epsilon_11) + (lame_lambda * trace_epsilon)\n",
    "      sigma_22 = 2 * (lame_mu * epsilon_22) + (lame_lambda * trace_epsilon)\n",
    "      sigma_12 = 2 * (lame_mu * epsilon_12)\n",
    "\n",
    "      internal_energy_density = 0.5 * (sigma_11 * epsilon_11 + 2 * sigma_12 * epsilon_12 + sigma_22 * epsilon_22)\n",
    "\n",
    "      internal_energy = BRIDGE.domain_volume * internal_energy_density.mean()\n",
    "\n",
    "      # Total Potential Energy\n",
    "      energy = internal_energy - external_energy\n",
    "\n",
    "      return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7eb30",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1770668091440,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "a5b7eb30"
   },
   "outputs": [],
   "source": [
    "class topology_optimization(Properties):\n",
    "    def __init__(self,\n",
    "                 u_model,\n",
    "                 v_model,\n",
    "                 GINN_model,\n",
    "                 n_opt_samples,\n",
    "                 training_hparams,\n",
    "                 enforce_density):\n",
    "        super().__init__(test_case=BRIDGE)\n",
    "        self.u_model = u_model.to(self.device)\n",
    "        self.v_model = v_model.to(self.device)\n",
    "        self.GINN_model = GINN_model.to(self.device)\n",
    "        self.n_opt_samples = n_opt_samples\n",
    "        self.enforce_density = enforce_density\n",
    "        self.density_exponent = training_hparams['density_exponent']\n",
    "\n",
    "    def compute_sensitivities(self,coords):\n",
    "        self.u_model.eval(); self.v_model.eval(); self.GINN_model.eval()\n",
    "        with torch.enable_grad():\n",
    "            densities = self.GINN_model(coords).detach()\n",
    "            densities, _ = self.enforce_density.apply(coords,densities, None,self.n_opt_samples,BRIDGE.domain)\n",
    "            densities.requires_grad_(True)\n",
    "\n",
    "            coords.requires_grad_(True)\n",
    "            u = self.u_model(coords)\n",
    "            v = self.v_model(coords)\n",
    "\n",
    "            grad_u = torch.autograd.grad(u, coords, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "            grad_v = torch.autograd.grad(v, coords, grad_outputs=torch.ones_like(v), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "            epsilon_11 = grad_u[:, 0]\n",
    "            epsilon_22 = grad_v[:, 1]\n",
    "            epsilon_12 = 0.5 * (grad_u[:, 1] + grad_v[:, 0])\n",
    "            trace_epsilon = epsilon_11 + epsilon_22\n",
    "\n",
    "            densities = densities.squeeze()\n",
    "            lame_lambda = self.lame_lambda * torch.ones_like(epsilon_11)\n",
    "            lame_lambda = densities.clamp(0.0, 1.0).pow(self.density_exponent)*lame_lambda\n",
    "\n",
    "            lame_mu = self.lame_mu * torch.ones_like(epsilon_11)\n",
    "            lame_mu = densities.clamp(0.0, 1.0).pow(self.density_exponent)*lame_mu\n",
    "\n",
    "            sigma_11 = 2 * (lame_mu * epsilon_11) + (lame_lambda * trace_epsilon)\n",
    "            sigma_22 = 2 * (lame_mu * epsilon_22) + (lame_lambda * trace_epsilon)\n",
    "            sigma_12 = 2 * (lame_mu * epsilon_12)\n",
    "\n",
    "            strain_energy_density = 0.5 * (sigma_11 * epsilon_11 + 2 * sigma_12 * epsilon_12 + sigma_22 * epsilon_22)\n",
    "\n",
    "            internal_energy = BRIDGE.domain_volume * strain_energy_density.mean()\n",
    "\n",
    "            loss = internal_energy\n",
    "\n",
    "            (d_rho,) = torch.autograd.grad(loss,densities, retain_graph=False, create_graph=False, allow_unused=False)\n",
    "            sensitivities = -d_rho  # match TF custom-gradient sign\n",
    "\n",
    "        return densities.detach(), sensitivities.detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_replicate_2d(t: torch.Tensor, rep: int) -> torch.Tensor:\n",
    "        \"\"\"Helper for sensitivity filter - following method from https://github.com/JonasZehn/ntopo/tree/main/ntopo \"\"\"\n",
    "        return F.pad(t, (rep, rep, rep, rep), mode=\"replicate\")\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_constant_2d(t: torch.Tensor, rep: int, value: float) -> torch.Tensor:\n",
    "        \"\"\"Helper for sensitivity filter - following method from https://github.com/JonasZehn/ntopo/tree/main/ntopo \"\"\"\n",
    "        return F.pad(t, (rep, rep, rep, rep), mode=\"constant\", value=value)\n",
    "\n",
    "    def apply_sensitivity_filter_2d(self,\n",
    "                                    coords: torch.Tensor,\n",
    "                                    old_densities: torch.Tensor,\n",
    "                                    sensitivities: torch.Tensor,\n",
    "                                    n_samples: Tuple[int,int],\n",
    "                                    domain: np.ndarray,\n",
    "                                    radius: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        PyTorch analogue of ntopo.apply_sensitivity_filter_3d:\n",
    "        - adopted from https://github.com/JonasZehn/ntopo/tree/main/ntopo .\n",
    "        \"\"\"\n",
    "        gamma = 1e-3\n",
    "        nx, ny = n_samples\n",
    "        N = nx * ny\n",
    "        assert coords.shape[0] == N\n",
    "\n",
    "        cell_width = (domain[1] - domain[0]) / nx\n",
    "        radius_space = radius * cell_width\n",
    "        fsize = 2*round(radius) + 1\n",
    "        rep = fsize // 2\n",
    "\n",
    "        dens = old_densities.view(ny, nx, 1).permute(2,0,1).unsqueeze(0)  # [1,1,ny,nx]\n",
    "        sens = sensitivities.view(ny, nx, 1).permute(2,0,1).unsqueeze(0)  # [1,1,ny,nx]\n",
    "        dens_p = self.pad_replicate_2d(dens, rep)\n",
    "        sens_p = self.pad_replicate_2d(sens, rep)\n",
    "\n",
    "        k = fsize\n",
    "        dens_patch = F.unfold(dens_p, kernel_size=k, stride=1)  # [1,k*k,ny*nx]\n",
    "        sens_patch = F.unfold(sens_p, kernel_size=k, stride=1)  # [1,k*k,ny*nx]\n",
    "\n",
    "        pos = coords.view(ny, nx, 2).permute(2,0,1).unsqueeze(0)  # [1,2,ny,nx]\n",
    "        pos_p = self.pad_constant_2d(pos, rep, value=-1000.0)\n",
    "        pos_patch = F.unfold(pos_p, kernel_size=k, stride=1)  # [1,2*k*k,ny*nx]\n",
    "        pos_patch = pos_patch.view(1, 2, k*k, ny*nx)\n",
    "        center = pos.view(1, 2, 1, ny*nx)\n",
    "\n",
    "        diff = pos_patch - center\n",
    "        dists = torch.sqrt((diff**2).sum(dim=1) + 1e-35)  # [1,k*k,ny*nx]\n",
    "        Hei = torch.clamp(radius_space - dists, min=0.0)\n",
    "\n",
    "        Heixic = Hei * dens_patch * sens_patch\n",
    "        sum_Heixic = Heixic.sum(dim=1)     # [1,ny*nx]\n",
    "        sum_Hei = Hei.sum(dim=1)           # [1,ny*nx]\n",
    "\n",
    "        old_r = old_densities.view(1, -1)  # [1,ny*nx]\n",
    "        div = torch.clamp(old_r, min=gamma) * sum_Hei\n",
    "        grads = (sum_Heixic / (div + 1e-35)).t()  # [ny*nx,1]\n",
    "        return grads\n",
    "\n",
    "    def apply_sensitivity_filter(self,coords, old_densities, sensitivities, n_samples, domain, radius):\n",
    "        return self.apply_sensitivity_filter_2d(coords, old_densities, sensitivities, n_samples, domain, radius)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_target_densities(self,\n",
    "                                 old_densities_list: List[torch.Tensor],\n",
    "                                 sensitivities_list: List[torch.Tensor],\n",
    "                                 sample_volume: float,\n",
    "                                 target_volume: float,\n",
    "                                 max_move: float,\n",
    "                                 damping_parameter: float):\n",
    "        \"\"\"\n",
    "        Compute target densities using OC update rule: derived from https://github.com/JonasZehn/ntopo/tree/main/ntopo\n",
    "        \"\"\"\n",
    "        total = sum([odi.numel() for odi in old_densities_list])\n",
    "        dv = sample_volume / float(total)\n",
    "\n",
    "        lb_list = [torch.clamp(odi - max_move, 0.0, 1.0) for odi in old_densities_list]\n",
    "        ub_list = [torch.clamp(odi + max_move, 0.0, 1.0) for odi in old_densities_list]\n",
    "\n",
    "        def targets_for_lambda(lmbd: float):\n",
    "            targets = []\n",
    "            flat_all = []\n",
    "            for odi, s, lb, ub in zip(old_densities_list, sensitivities_list, lb_list, ub_list):\n",
    "                Bi = s / (-(dv * lmbd) + 1e-20)\n",
    "                tgt = odi * torch.pow(torch.clamp(Bi, min=1e-20), damping_parameter)\n",
    "                tgt = torch.maximum(lb, torch.minimum(ub, tgt))\n",
    "                tgt = torch.clamp(tgt, 0.0, 1.0)\n",
    "                targets.append(tgt)\n",
    "                flat_all.append(tgt.reshape(-1))\n",
    "            vol = sample_volume * torch.mean(torch.cat(flat_all, dim=0))\n",
    "            return vol, targets\n",
    "\n",
    "        lam_lo, lam_hi = 0.0, 1e9\n",
    "        for _ in range(60):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            vol_mid, _ = targets_for_lambda(lam_mid)\n",
    "            if (lam_hi - lam_lo) / (lam_hi + lam_lo + 1e-12) < 1e-3:\n",
    "                break\n",
    "            if vol_mid > target_volume:\n",
    "                lam_lo = lam_mid\n",
    "            else:\n",
    "                lam_hi = lam_mid\n",
    "\n",
    "        _, targets = targets_for_lambda(0.5 * (lam_lo + lam_hi))\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52deca8",
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1770668091532,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "f52deca8"
   },
   "outputs": [],
   "source": [
    "class model_training:\n",
    "    def __init__(self,\n",
    "                 u_model,\n",
    "                 v_model,\n",
    "                 density_GINN_model,\n",
    "                 SDF_GINN_model,\n",
    "                 training_hparams,\n",
    "                 topo_hparams,\n",
    "                 constraint_hparams,\n",
    "                 test_case,\n",
    "                 loss_weight_hparams,\n",
    "                 GINN_hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------- Device ----------------\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "\n",
    "        # ---------------- Models ----------------\n",
    "        self.u_model = u_model.to(self.device)\n",
    "        self.v_model = v_model.to(self.device)\n",
    "        self.density_GINN_model = density_GINN_model.to(self.device)\n",
    "        self.SDF_GINN_model = SDF_GINN_model.to(self.device)\n",
    "\n",
    "        # ---------------- Hparams ----------------\n",
    "        self.topo_hparams = topo_hparams\n",
    "        self.training_hparams = training_hparams\n",
    "        self.test_case = test_case\n",
    "        self.loss_weight_hparams = loss_weight_hparams\n",
    "        self.GINN_hparams = GINN_hparams\n",
    "\n",
    "        self.save_path = topo_hparams['save_path']\n",
    "        self.volume_ratio = topo_hparams.get('volume_ratio', None)\n",
    "        self.lr_PINN = topo_hparams['lr_PINN']\n",
    "        self.lr_GINN = topo_hparams['lr_GINN']\n",
    "        self.save_interval = topo_hparams['save_interval']\n",
    "        self.filter_radius = topo_hparams['filter_radius']\n",
    "        self.n_opt_iterations = topo_hparams['n_opt_iterations']\n",
    "        self.n_sim_iterations = topo_hparams['n_sim_iterations']\n",
    "        self.n_pre_training_iterations_PINN = topo_hparams['n_pre_training_iterations_PINN']\n",
    "        self.n_pre_training_iterations_GINN = topo_hparams['n_pre_training_iterations_GINN']\n",
    "        self.n_opt_batches = topo_hparams['n_opt_batches']\n",
    "        self.seed = topo_hparams['seed']\n",
    "        self.refine_study = training_hparams[\"refine_study\"]\n",
    "        self.batch_size = training_hparams['batch_size']\n",
    "        self.num_points = training_hparams['total_sample_points']\n",
    "        self.sigma_allow = float(topo_hparams.get('sigma_allow', 1.0))\n",
    "        self.stress_percentile = float(topo_hparams.get('stress_percentile', 0.995))\n",
    "        self.rho_mask_threshold = float(topo_hparams.get('rho_mask_threshold', 0.6))\n",
    "        self.stress_tol = float(topo_hparams.get('stress_tol', 0.02))\n",
    "        self.vol_frac_min = float(topo_hparams.get('vol_frac_min', 0.02))\n",
    "        self.vol_frac_max = float(topo_hparams.get('vol_frac_max', 0.98))\n",
    "        self.vol_step_min = float(topo_hparams.get('vol_step_min', 0.005))\n",
    "        self.vol_step_max = float(topo_hparams.get('vol_step_max', 0.05))\n",
    "        self.alpha_decrease = float(topo_hparams.get('alpha_decrease', 0.50))\n",
    "        self.beta_increase = float(topo_hparams.get('beta_increase', 0.25))\n",
    "        self.plot_threshold = float(topo_hparams.get('rho_treshold', 0.3))\n",
    "        self.stress_metric = str(topo_hparams.get(\"stress_metric\", \"percentile\"))\n",
    "        self.ks_rho = float(topo_hparams.get(\"ks_rho\", 50.0))\n",
    "        self.ks_size_correction = bool(topo_hparams.get(\"ks_size_correction\", True))\n",
    "\n",
    "        if getattr(self.density_GINN_model, \"volume_ratio\", None) is None:\n",
    "            self.density_GINN_model.volume_ratio = 0.5\n",
    "\n",
    "        self.sigma_ema = None\n",
    "        self.sigma_ema_alpha = 0.2\n",
    "\n",
    "        self.enforce_density = Density_Constraints(\n",
    "            constraint_hparams,\n",
    "            self.test_case,\n",
    "            self.test_case.interfaces.is_inside_interface_thickness,\n",
    "            self.test_case.interfaces.is_inside_prohibited_region\n",
    "        )\n",
    "\n",
    "        # ---------------- ALM setup for first optimization stage ----------------\n",
    "        self.use_objective_function = bool(loss_weight_hparams.get('objective_function', False))\n",
    "        self.objective_losses = list(loss_weight_hparams.get('objective_losses', []))\n",
    "\n",
    "        all_geom_loss_keys = [\n",
    "            'Eikonal Loss',\n",
    "            'Interface Loss',\n",
    "            'Envelope Loss',\n",
    "            'Connectivity Loss',\n",
    "            'Prescribed Normals Loss',\n",
    "            'Prescribed Thickness Loss',\n",
    "            'Prohibited Region Loss',\n",
    "            'Smoothness Loss',\n",
    "            'Holes Loss',\n",
    "        ]\n",
    "\n",
    "        if self.use_objective_function:\n",
    "            constraint_keys = [k for k in all_geom_loss_keys if k not in self.objective_losses]\n",
    "        else:\n",
    "            constraint_keys = [\n",
    "                'Envelope Loss',\n",
    "                'Connectivity Loss',\n",
    "                'Prescribed Normals Loss',\n",
    "                'Prescribed Thickness Loss',\n",
    "                'Prohibited Region Loss',\n",
    "                'Smoothness Loss',\n",
    "                'Holes Loss',\n",
    "            ]\n",
    "\n",
    "        self.scalar_keys = ['Objective'] + constraint_keys\n",
    "\n",
    "        lambda_init = {'Objective': 1.0}\n",
    "\n",
    "        if 'Eikonal Loss' in constraint_keys:\n",
    "            lambda_init['Eikonal Loss'] = GINN_hparams.get('eikonal_loss_weight', 0.0)\n",
    "        if 'Interface Loss' in constraint_keys:\n",
    "            lambda_init['Interface Loss'] = GINN_hparams.get('interface_loss_weight', 1.0)\n",
    "        if 'Envelope Loss' in constraint_keys:\n",
    "            lambda_init['Envelope Loss'] = GINN_hparams.get('envelope_loss_weight', 1.0)\n",
    "        if 'Connectivity Loss' in constraint_keys:\n",
    "            lambda_init['Connectivity Loss'] = GINN_hparams.get('connectivity_loss_weight', 100.0)\n",
    "        if 'Prescribed Normals Loss' in constraint_keys:\n",
    "            lambda_init['Prescribed Normals Loss'] = GINN_hparams.get('prescribed_normals_loss_weight', 1.0)\n",
    "        if 'Prescribed Thickness Loss' in constraint_keys:\n",
    "            lambda_init['Prescribed Thickness Loss'] = GINN_hparams.get('prescribed_thickness_loss_weight', 1.0)\n",
    "        if 'Prohibited Region Loss' in constraint_keys:\n",
    "            lambda_init['Prohibited Region Loss'] = GINN_hparams.get('prohibited_region_loss_weight', 1.0)\n",
    "        if 'Smoothness Loss' in constraint_keys:\n",
    "            lambda_init['Smoothness Loss'] = GINN_hparams.get('smoothness_loss_weight', 1e-4)\n",
    "        if 'Holes Loss' in constraint_keys:\n",
    "            lambda_init['Holes Loss'] = GINN_hparams.get('holes_loss_weight', 100.0)\n",
    "\n",
    "        self.alm = ALM(\n",
    "            loss_keys=self.scalar_keys,\n",
    "            objective_key='Objective',\n",
    "            lambda_dict=lambda_init,\n",
    "            alpha=loss_weight_hparams['alpha'],\n",
    "            gamma=loss_weight_hparams['gamma'],\n",
    "            epsilon=loss_weight_hparams['epsilon'],\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ---------------- ALM setup for second optimization stage ----------------\n",
    "        self.scalar_keys_rho = [\n",
    "            'Topo Objective',\n",
    "            'Density Envelope Loss',\n",
    "            'Density Connectivity Loss'\n",
    "        ]\n",
    "\n",
    "        lambda_init_rho = {\n",
    "            'Topo Objective':            1.0,\n",
    "            'Density Envelope Loss':     GINN_hparams.get('envelope_loss_weight_density', 1.0),\n",
    "            'Density Connectivity Loss': GINN_hparams.get('connectivity_loss_weight_density', 100.0),\n",
    "        }\n",
    "\n",
    "        self.alm_rho = ALM(\n",
    "            loss_keys=self.scalar_keys_rho,\n",
    "            objective_key='Topo Objective',\n",
    "            lambda_dict=lambda_init_rho,\n",
    "            alpha=loss_weight_hparams['alpha'],\n",
    "            gamma=loss_weight_hparams['gamma'],\n",
    "            epsilon=loss_weight_hparams['epsilon'],\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ---------------- Gradient clipping ----------------\n",
    "        self.grad_clipper = AutoClip(\n",
    "            grad_clipping_on=training_hparams[\"grad_clipping_on\"],\n",
    "            grad_clip=training_hparams[\"grad_clip\"],\n",
    "            auto_clip_on=training_hparams[\"auto_clip_on\"],\n",
    "            auto_clip_percentile=training_hparams[\"auto_clip_percentile\"],\n",
    "            auto_clip_min_len=training_hparams[\"auto_clip_min_len\"],\n",
    "            auto_clip_hist_len=training_hparams[\"auto_clip_hist_len\"]\n",
    "        )\n",
    "\n",
    "        # ---------------- Samplers ----------------\n",
    "        self.point_sampler = Point_Sampler(\n",
    "            test_case.domain,\n",
    "            test_case.interfaces,\n",
    "            num_points_domain=self.num_points,\n",
    "            num_points_interface=0\n",
    "        )\n",
    "\n",
    "        inside_env = make_inside_envelope_fn_from_domain(\n",
    "            domain=self.test_case.domain, nx=self.test_case.dim, device=self.device\n",
    "        )\n",
    "\n",
    "        # PH set-up\n",
    "        self.PH = PH(\n",
    "            nx=test_case.dim,\n",
    "            bounds=test_case.domain,\n",
    "            model=self.SDF_GINN_model,\n",
    "            n_grid_points=128,\n",
    "            iso_level=0.0,\n",
    "            target_betti=[1, 0, 0],\n",
    "            maxdim=1,\n",
    "            is_density=False,\n",
    "            inside_envelope_fn=inside_env,\n",
    "            group_size_fwd_no_grad=32768,\n",
    "            add_frame=True,\n",
    "            hole_level=0.06,\n",
    "            test_case=self.test_case\n",
    "        )\n",
    "\n",
    "        # PH for the density field\n",
    "        self.PH_rho = PH(\n",
    "            nx=test_case.dim,\n",
    "            bounds=test_case.domain,\n",
    "            model=self.density_GINN_model,\n",
    "            n_grid_points=128,\n",
    "            iso_level=self.plot_threshold,\n",
    "            target_betti=[1, 0, 0],\n",
    "            maxdim=1,\n",
    "            is_density=True,\n",
    "            inside_envelope_fn=inside_env,\n",
    "            group_size_fwd_no_grad=32768,\n",
    "            add_frame=True,\n",
    "            hole_level=0.06,\n",
    "            test_case=self.test_case\n",
    "        )\n",
    "\n",
    "        interface_points = BRIDGE.interfaces.sample_points_from_all_interfaces(\n",
    "            10001, output_type='torch_tensor'\n",
    "        )\n",
    "        self.boundary_sampler = Boundary_Sampler(\n",
    "            dim=BRIDGE.dim,\n",
    "            bounds=BRIDGE.domain,\n",
    "            model=self.SDF_GINN_model,\n",
    "            x_interface=interface_points,\n",
    "            n_points_surface=10001,\n",
    "            interface_cutoff=0.05\n",
    "        )\n",
    "\n",
    "        self.boundary_sampler_rho = Boundary_Sampler(\n",
    "            dim=self.test_case.dim,\n",
    "            bounds=self.test_case.domain,\n",
    "            model=self.density_GINN_model,\n",
    "            x_interface=interface_points,\n",
    "            n_points_surface=10001,\n",
    "            interface_cutoff=0.05,\n",
    "            level_set = 0.5,\n",
    "        )\n",
    "\n",
    "        # History buffers\n",
    "        self.hist_iters = []\n",
    "        self.hist_compliance = []\n",
    "        self.hist_volume = []\n",
    "        self.hist_sigma_metric = []\n",
    "        self.hist_sigma_metric_KS = []\n",
    "        self.ginn_hist_iters = []\n",
    "        self.ginn_hist_eik = []\n",
    "        self.ginn_hist_env = []\n",
    "        self.ginn_hist_connect = []\n",
    "        self.ginn_hist_hole = []\n",
    "        self.ginn_hist_int = []\n",
    "        self.ginn_hist_norm = []\n",
    "        self.ginn_hist_thick = []\n",
    "        self.ginn_hist_prohib = []\n",
    "        self.ginn_hist_smooth = []\n",
    "        self.ginn_hist_total = []\n",
    "        self.topo_hist_iters = []\n",
    "        self.topo_hist_topo = []\n",
    "        self.topo_hist_env_rho = []\n",
    "        self.topo_hist_conn_rho = []\n",
    "        self.topo_hist_total = []\n",
    "        self.topo_step_counter = 0\n",
    "        self.time_ginn_pretrain = 0.0\n",
    "        self.time_topology_phase = 0.0\n",
    "        self.time_density_updates = 0.0\n",
    "        self.time_total = 0.0\n",
    "        self._density_update_calls = 0\n",
    "        self._density_update_time_buffer = []\n",
    "\n",
    "        # ---------------- paths ----------------\n",
    "        self.csv_time_path = os.path.join(self.save_path, \"timing.csv\")\n",
    "        self.csv_ginn_loss_path = os.path.join(self.save_path, \"ginn_losses.csv\")\n",
    "        self.csv_topo_loss_path = os.path.join(self.save_path, \"topo_density_losses.csv\")\n",
    "        self.csv_opt_metrics_path = os.path.join(self.save_path, \"topo_metrics.csv\")\n",
    "        self.csv_opt_loss_path = os.path.join(self.save_path, \"optimization_losses.csv\")\n",
    "        self.csv_eval_metrics_path = os.path.join(self.save_path, \"evaluation_metrics.csv\")\n",
    "\n",
    "\n",
    "        self.eval_metrics_enable = bool(self.topo_hparams.get(\"eval_metrics_enable\", True))\n",
    "        self.eval_grid_nx = int(self.topo_hparams.get(\"eval_metrics_grid_nx\", 256))\n",
    "        self.eval_grid_ny = int(self.topo_hparams.get(\"eval_metrics_grid_ny\", 128))\n",
    "        self.eval_batch = int(self.topo_hparams.get(\"eval_metrics_batch\", 200_000))\n",
    "        self.eval_boundary_n = int(self.topo_hparams.get(\"eval_metrics_boundary_n\", 2_000))\n",
    "        self.eval_interface_n = int(self.topo_hparams.get(\"eval_metrics_interface_n\", 1_024))\n",
    "        self.eval_max_boundary_pts = int(self.topo_hparams.get(\"eval_metrics_max_boundary_pts\", 20_000))\n",
    "        self.eval_cdist_chunk = int(self.topo_hparams.get(\"eval_metrics_cdist_chunk\", 4_096))\n",
    "\n",
    "\n",
    "        # ---------------- Model checkpoint interval ----------------\n",
    "        self.model_save_interval = int(topo_hparams.get(\"model_save_interval\", self.save_interval))\n",
    "\n",
    "\n",
    "    # ------ helpers ---------------------\n",
    "\n",
    "    def loss_ramp_up(self, epoch: int, start_epoch: int, ramp_epochs: int) -> float:\n",
    "        \"\"\"Gradually ramps up the strength of a loss to avoid numerical issues\n",
    "        when introducing a new loss during training.\"\"\"\n",
    "        if epoch < start_epoch:\n",
    "            return 0.0\n",
    "        return float(min(1.0, (epoch - start_epoch + 1) / max(1, ramp_epochs)))\n",
    "\n",
    "    _compute_eval_metrics_2d = compute_eval_metrics_2d\n",
    "    _maybe_log_eval_metrics = maybe_log_eval_metrics\n",
    "    _log_timing = log_timing\n",
    "    _log_ginn_losses_csv = log_ginn_losses_csv\n",
    "    _log_topo_density_losses_csv = log_topo_density_losses_csv\n",
    "    _log_optimization_metrics_csv = log_optimization_metrics_csv\n",
    "    _log_optimization_losses_csv = log_optimization_losses_csv\n",
    "    _record_density_update_time = record_density_update_time\n",
    "    _save_models = save_models\n",
    "\n",
    "    def _compute_losses(self, coords, epoch):\n",
    "        geometry = GINN_losses(\n",
    "            self.SDF_GINN_model,\n",
    "            self.test_case,\n",
    "            self.GINN_hparams,\n",
    "            self.PH,\n",
    "            self.boundary_sampler,\n",
    "            self.enforce_density\n",
    "        )\n",
    "\n",
    "        w = self.GINN_hparams\n",
    "        ph_ramp = self.loss_ramp_up(epoch, start_epoch=0, ramp_epochs=550)\n",
    "\n",
    "        eik_raw     = geometry.eikonal_loss(coords)\n",
    "        int_raw     = geometry.interface_loss()\n",
    "        env_raw     = geometry.design_envelope_loss()\n",
    "        conn_raw    = geometry.connectivity_loss()\n",
    "        norm_raw    = geometry.surface_normal_loss()\n",
    "        thick_raw   = geometry.prescribed_thickness_loss(coords)\n",
    "        prohib_raw  = geometry.prohibited_region_loss(coords)\n",
    "        smooth0_raw = geometry.smoothness_loss(epoch)\n",
    "        holes_raw   = geometry.holes_loss()\n",
    "\n",
    "        eik     = eik_raw                                  * w['eikonal_loss_weight']\n",
    "        intr    = int_raw                                  * w['interface_loss_weight']\n",
    "        env     = env_raw                                  * w['envelope_loss_weight']\n",
    "        conn    = conn_raw                                 * (w['connectivity_loss_weight'] * ph_ramp)\n",
    "        norm    = norm_raw                                 * w['prescribed_normals_loss_weight']\n",
    "        thick   = thick_raw                                * w['prescribed_thickness_loss_weight']\n",
    "        prohib  = prohib_raw                               * w['prohibited_region_loss_weight']\n",
    "        smooth  = self.loss_ramp_up(\n",
    "                        epoch,\n",
    "                        w['curv_start_epoch'],\n",
    "                        w['curv_ramp_epochs']\n",
    "                  ) * smooth0_raw * w['smoothness_loss_weight']\n",
    "        holes   = holes_raw                                * (w['holes_loss_weight'] * ph_ramp)\n",
    "\n",
    "        losses_all = {\n",
    "            'Eikonal Loss':              torch.relu(eik),\n",
    "            'Interface Loss':            torch.relu(intr),\n",
    "            'Envelope Loss':             torch.relu(env),\n",
    "            'Connectivity Loss':         torch.relu(conn),\n",
    "            'Prescribed Normals Loss':   torch.relu(norm),\n",
    "            'Prescribed Thickness Loss': torch.relu(thick),\n",
    "            'Prohibited Region Loss':    torch.relu(prohib),\n",
    "            'Smoothness Loss':           torch.relu(smooth),\n",
    "            'Holes Loss':                torch.relu(holes),\n",
    "        }\n",
    "\n",
    "        # --------- Build single objective scalar ----------\n",
    "        if self.use_objective_function and len(self.objective_losses) > 0:\n",
    "            objective = None\n",
    "            for name in self.objective_losses:\n",
    "                if name not in losses_all:\n",
    "                    raise KeyError(\n",
    "                        f\"Objective loss '{name}' not found. \"\n",
    "                        f\"Available: {list(losses_all.keys())}\"\n",
    "                    )\n",
    "                objective = losses_all[name] if objective is None else (objective + losses_all[name])\n",
    "        else:\n",
    "            objective = losses_all['Eikonal Loss'] + losses_all['Interface Loss']\n",
    "\n",
    "        losses = {'Objective': torch.relu(objective)}\n",
    "\n",
    "        for key in self.scalar_keys:\n",
    "            if key == 'Objective':\n",
    "                continue\n",
    "            if key not in losses_all:\n",
    "                raise KeyError(\n",
    "                    f\"Constraint key '{key}' not found in losses_all. \"\n",
    "                    f\"Available: {list(losses_all.keys())}\"\n",
    "                )\n",
    "            losses[key] = losses_all[key]\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _safe_clip_ratio(v, lo, hi, eps=1e-3):\n",
    "        lo = max(lo, eps)\n",
    "        hi = min(hi, 1.0 - eps)\n",
    "        return float(np.clip(v, lo, hi))\n",
    "\n",
    "    def _estimate_compliance_internal_energy(self, n_opt_samples):\n",
    "        self.u_model.eval()\n",
    "        self.v_model.eval()\n",
    "        self.density_GINN_model.eval()\n",
    "\n",
    "        nx, ny = n_opt_samples\n",
    "        xs = get_grid_centers(BRIDGE.domain, [nx, ny]).astype(np.float32)\n",
    "        xt = torch.tensor(xs, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "\n",
    "        rho = self.density_GINN_model(xt)\n",
    "        rho, _ = self.enforce_density.apply(xt, rho, None, n_opt_samples, BRIDGE.domain)\n",
    "        r = rho.view(-1).clamp(0.0, 1.0)\n",
    "\n",
    "        u = self.u_model(xt)\n",
    "        v = self.v_model(xt)\n",
    "\n",
    "        gu = torch.autograd.grad(u, xt, grad_outputs=torch.ones_like(u),\n",
    "                                 create_graph=False, retain_graph=True)[0]\n",
    "        gv = torch.autograd.grad(v, xt, grad_outputs=torch.ones_like(v),\n",
    "                                 create_graph=False, retain_graph=False)[0]\n",
    "\n",
    "        eps11 = gu[:, 0]\n",
    "        eps22 = gv[:, 1]\n",
    "        eps12 = 0.5 * (gu[:, 1] + gv[:, 0])\n",
    "        tr = eps11 + eps22\n",
    "\n",
    "        p = float(self.training_hparams['density_exponent'])\n",
    "        lam0 = Properties(BRIDGE).lame_lambda\n",
    "        mu0 = Properties(BRIDGE).lame_mu\n",
    "        lam = r.pow(p) * lam0\n",
    "        mu = r.pow(p) * mu0\n",
    "\n",
    "        s11 = 2.0 * mu * eps11 + lam * tr\n",
    "        s22 = 2.0 * mu * eps22 + lam * tr\n",
    "        s12 = 2.0 * mu * eps12\n",
    "\n",
    "        sed = 0.5 * (s11 * eps11 + 2.0 * s12 * eps12 + s22 * eps22)\n",
    "        internal_energy = BRIDGE.domain_volume * sed.mean()\n",
    "        return float(internal_energy.detach().cpu().item())\n",
    "\n",
    "\n",
    "    _save_training_curves  = save_training_curves\n",
    "    _save_ginn_training_curves = save_ginn_training_curves\n",
    "    _save_topology_density_curves = save_topology_density_curves\n",
    "    _save_stress_histogram = save_stress_histogram\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- Stress/volume measurement ----------------\n",
    "    def _measure_stress_volume_percentile_2d(self):\n",
    "        self.u_model.eval()\n",
    "        self.v_model.eval()\n",
    "        self.density_GINN_model.eval()\n",
    "\n",
    "        u_img, v_img, sigma_img, rho_img = predict_uv_sigma_image_2d_binary(\n",
    "            self.u_model,\n",
    "            self.v_model,\n",
    "            self.density_GINN_model,\n",
    "            BRIDGE.domain,\n",
    "            test_case=self.test_case,\n",
    "            rho_threshold_plot=self.plot_threshold\n",
    "        )\n",
    "\n",
    "        solid = (rho_img >= self.plot_threshold)\n",
    "        if not np.any(solid):\n",
    "            flat = rho_img.reshape(-1)\n",
    "            t = float(np.quantile(flat, 0.5))\n",
    "            solid = (rho_img >= t)\n",
    "\n",
    "        sigma_vals = sigma_img[solid] if np.any(solid) else sigma_img.reshape(-1)\n",
    "\n",
    "        q = float(np.clip(self.stress_percentile, 0.0, 1.0))\n",
    "        sigma_metric = float(np.quantile(sigma_vals, q))\n",
    "        sigma_max = float(np.max(sigma_vals))\n",
    "\n",
    "        current_volume = BRIDGE.domain_volume * (\n",
    "            float(np.count_nonzero(solid)) / solid.size\n",
    "        )\n",
    "        return sigma_metric, sigma_max, current_volume\n",
    "\n",
    "    def _measure_stress_volume_KS_2d(self):\n",
    "        self.u_model.eval()\n",
    "        self.v_model.eval()\n",
    "        self.density_GINN_model.eval()\n",
    "\n",
    "        u_img, v_img, sigma_img, rho_img = predict_uv_sigma_image_2d_binary(\n",
    "            self.u_model,\n",
    "            self.v_model,\n",
    "            self.density_GINN_model,\n",
    "            BRIDGE.domain,\n",
    "            test_case=self.test_case,\n",
    "            rho_threshold_plot=self.plot_threshold\n",
    "        )\n",
    "\n",
    "        solid = (rho_img >= self.plot_threshold)\n",
    "        if not np.any(solid):\n",
    "            flat = rho_img.reshape(-1)\n",
    "            t = float(np.quantile(flat, 0.5))\n",
    "            solid = (rho_img >= t)\n",
    "\n",
    "        sigma_vals = sigma_img[solid]\n",
    "        if sigma_vals.size == 0:\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        volume_abs = BRIDGE.domain_volume * (\n",
    "            float(np.count_nonzero(solid)) / solid.size\n",
    "        )\n",
    "\n",
    "        s = sigma_vals.astype(np.float64)\n",
    "        g = s / float(self.sigma_allow) - 1.0\n",
    "        gmax = float(np.max(g))\n",
    "\n",
    "        lse = gmax + (1.0 / self.ks_rho) * float(\n",
    "            np.log(np.sum(np.exp(self.ks_rho * (g - gmax))) + 1e-300)\n",
    "        )\n",
    "        if self.ks_size_correction:\n",
    "            lse -= (1.0 / self.ks_rho) * float(np.log(s.size))\n",
    "\n",
    "        ks_val = lse\n",
    "        sigma_metric = float(self.sigma_allow) * (1.0 + ks_val)\n",
    "        sigma_max = float(np.max(s))\n",
    "\n",
    "        return sigma_metric, sigma_max, volume_abs\n",
    "\n",
    "    def _measure_stress_volume_sigma_max_2d(self):\n",
    "        _, sigma_max, vol_abs = self._measure_stress_volume_percentile_2d()\n",
    "        sigma_metric = sigma_max\n",
    "        return sigma_metric, sigma_max, vol_abs\n",
    "\n",
    "    def _measure_stress_volume_2d(self):\n",
    "        metric = str(getattr(self, \"stress_metric\", \"percentile\")).lower()\n",
    "\n",
    "        if metric == \"ks\":\n",
    "            return self._measure_stress_volume_KS_2d()\n",
    "        elif metric == \"percentile\":\n",
    "            return self._measure_stress_volume_percentile_2d()\n",
    "        elif metric == \"sigma_max\":\n",
    "            return self._measure_stress_volume_sigma_max_2d()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown stress metric: {metric}. \"\n",
    "                f\"Choose between 'KS' and 'percentile'.\"\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # training steps\n",
    "    # ----------------------------------------------------------------------\n",
    "    def shape_generation_step(self, optimizer, epoch):\n",
    "        batch_points = next(self.point_sampler)\n",
    "        dataset = TensorDataset(batch_points)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for _, pts in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            points = pts[0].to(self.device)\n",
    "\n",
    "            self.PH.invalidate_cache()\n",
    "\n",
    "            losses = self._compute_losses(points.clone().detach().requires_grad_(True), epoch)\n",
    "\n",
    "            total_loss = self.alm.build(losses)\n",
    "            total_loss.backward()\n",
    "\n",
    "            if getattr(self, \"grad_clipper\", None) is not None and self.grad_clipper.grad_clip_enabled:\n",
    "                total_sq = 0.0\n",
    "                for p in self.SDF_GINN_model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        g = p.grad.data\n",
    "                        if torch.isfinite(g).all():\n",
    "                            total_sq += g.float().pow(2).sum().item()\n",
    "                grad_norm = math.sqrt(total_sq)\n",
    "                self.grad_clipper.update_gradient_norm_history(grad_norm)\n",
    "                clip_val = self.grad_clipper.get_clip_value()\n",
    "                if np.isfinite(clip_val):\n",
    "                    torch.nn.utils.clip_grad_norm_(self.SDF_GINN_model.parameters(), clip_val)\n",
    "\n",
    "            optimizer.step()\n",
    "            self.alm.update(losses)\n",
    "\n",
    "\n",
    "    def density_PINN_initialization_step(self, coords, density_GINN_optimizer):\n",
    "\n",
    "        if self.refine_study == True: # initialize geometry from existing geometry\n",
    "            self.density_GINN_model.train()\n",
    "            self.SDF_GINN_model.eval()\n",
    "\n",
    "            path = \"/content/gdrive/Othercomputers/My Mac/Code/hard_GINN_results/model-000250.pt\"\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.SDF_GINN_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            sdf_density = self.SDF_GINN_model(coords)\n",
    "\n",
    "            sdf_density = torch.sigmoid(-1000 * sdf_density).detach()\n",
    "            topo_init = torch.clamp(sdf_density, 0.0, 0.7)\n",
    "\n",
    "            density_pred = self.density_GINN_model(coords)\n",
    "            loss = F.mse_loss(density_pred, topo_init)\n",
    "\n",
    "            density_GINN_optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            density_GINN_optimizer.step()\n",
    "\n",
    "        else: # Normal training setup\n",
    "            self.density_GINN_model.train()\n",
    "            self.SDF_GINN_model.eval()\n",
    "\n",
    "            SDF_GINN_initial_density = self.SDF_GINN_model(coords)\n",
    "            SDF_GINN_initial_density = torch.sigmoid(-1000 * SDF_GINN_initial_density).detach()\n",
    "            topo_initial_density = torch.clamp(SDF_GINN_initial_density, 0.0, 0.5)\n",
    "\n",
    "            density_GINN_density = self.density_GINN_model(coords)\n",
    "            initial_density_loss = F.mse_loss(density_GINN_density, topo_initial_density)\n",
    "\n",
    "            density_GINN_optimizer.zero_grad(set_to_none=True)\n",
    "            initial_density_loss.backward()\n",
    "            density_GINN_optimizer.step()\n",
    "\n",
    "    def PINN_update_step(self,\n",
    "                         coords,\n",
    "                         PINN_optimizer,\n",
    "                         n_opt_samples):\n",
    "        self.u_model.train()\n",
    "        self.v_model.train()\n",
    "        self.density_GINN_model.eval()\n",
    "\n",
    "        physics_loss = PINN_losses(\n",
    "            self.u_model,\n",
    "            self.v_model,\n",
    "            self.density_GINN_model,\n",
    "            n_opt_samples,\n",
    "            self.training_hparams,\n",
    "            self.enforce_density\n",
    "        )\n",
    "\n",
    "        PINN_optimizer.zero_grad(set_to_none=True)\n",
    "        loss = physics_loss.ritz_loss(coords)\n",
    "        loss.backward()\n",
    "        PINN_optimizer.step()\n",
    "        return None\n",
    "\n",
    "    def topology_optimization_step(self,\n",
    "                                   density_GINN_optimizer,\n",
    "                                   coords,\n",
    "                                   target_densities):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        self.density_GINN_model.train()\n",
    "        density_GINN_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        densities = self.density_GINN_model(coords)\n",
    "        topology_optim_loss = F.mse_loss(densities, target_densities)\n",
    "\n",
    "        self.PH_rho.invalidate_cache()\n",
    "\n",
    "        geometry_rho = GINN_losses(\n",
    "            self.density_GINN_model,\n",
    "            self.test_case,\n",
    "            self.GINN_hparams,\n",
    "            self.PH_rho,\n",
    "            self.boundary_sampler,\n",
    "            self.enforce_density\n",
    "        )\n",
    "\n",
    "        env_rho = geometry_rho.design_envelope_loss_from_density(\n",
    "            density_model=self.density_GINN_model,\n",
    "            iso_level=float(self.plot_threshold)\n",
    "        )\n",
    "\n",
    "        conn_rho = geometry_rho.connectivity_loss_from_density(\n",
    "            density_model=self.density_GINN_model,\n",
    "            iso_level=float(self.plot_threshold)\n",
    "        )\n",
    "\n",
    "        losses_rho = {\n",
    "            'Topo Objective':             torch.relu(topology_optim_loss),\n",
    "            'Density Envelope Loss':      torch.relu(env_rho),\n",
    "            'Density Connectivity Loss':  torch.relu(conn_rho),\n",
    "        }\n",
    "\n",
    "        total_loss = self.alm_rho.build(losses_rho)\n",
    "        self.topo_step_counter += 1\n",
    "        self.topo_hist_iters.append(self.topo_step_counter)\n",
    "        self.topo_hist_topo.append(float(topology_optim_loss.item()))\n",
    "        self.topo_hist_env_rho.append(float(env_rho.item()))\n",
    "        self.topo_hist_conn_rho.append(float(conn_rho.item()))\n",
    "        total_raw = float(topology_optim_loss.item() + env_rho.item() + conn_rho.item())\n",
    "        self.topo_hist_total.append(total_raw)\n",
    "\n",
    "        self._log_topo_density_losses_csv(\n",
    "            step=self.topo_step_counter,\n",
    "            topo_obj=topology_optim_loss.item(),\n",
    "            env_rho=env_rho.item(),\n",
    "            conn_rho=conn_rho.item(),\n",
    "            total_raw=total_raw\n",
    "        )\n",
    "        # -------------------------------------------\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        if getattr(self, \"grad_clipper\", None) is not None and self.grad_clipper.grad_clip_enabled:\n",
    "            total_sq = 0.0\n",
    "            for p in self.density_GINN_model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    g = p.grad.data\n",
    "                    if torch.isfinite(g).all():\n",
    "                        total_sq += g.float().pow(2).sum().item()\n",
    "            grad_norm = math.sqrt(total_sq)\n",
    "            self.grad_clipper.update_gradient_norm_history(grad_norm)\n",
    "            clip_val = self.grad_clipper.get_clip_value()\n",
    "            if np.isfinite(clip_val):\n",
    "                torch.nn.utils.clip_grad_norm_(self.density_GINN_model.parameters(), clip_val)\n",
    "\n",
    "        density_GINN_optimizer.step()\n",
    "\n",
    "        self.alm_rho.update(losses_rho)\n",
    "\n",
    "        dt = time.perf_counter() - t0\n",
    "        self._record_density_update_time(dt)\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # main driver\n",
    "    # ----------------------------------------------------------------------\n",
    "    def generate_geometry(self):\n",
    "        import os\n",
    "        import time\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        set_random_seed(self.seed)\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "        t_total0 = time.perf_counter()\n",
    "\n",
    "        DEFAULT_N_SIM_SAMPLES_2D = 150 * 50\n",
    "        DEFAULT_N_OPT_SAMPLES_2D = 150 * 50\n",
    "\n",
    "        n_sim_samples = get_default_sample_counts(BRIDGE.domain, DEFAULT_N_SIM_SAMPLES_2D)\n",
    "        n_opt_samples = get_default_sample_counts(BRIDGE.domain, DEFAULT_N_OPT_SAMPLES_2D)\n",
    "\n",
    "        PINN_model_params_list = list(self.u_model.parameters()) + list(self.v_model.parameters())\n",
    "        PINN_optimizer = torch.optim.Adam(PINN_model_params_list, lr=self.lr_PINN, betas=(0.9, 0.99))\n",
    "        density_GINN_optimizer = torch.optim.Adam(self.density_GINN_model.parameters(), lr=self.lr_GINN, betas=(0.8, 0.9))\n",
    "        SDF_GINN_optimizer = torch.optim.Adam(self.SDF_GINN_model.parameters(), lr=self.lr_GINN)\n",
    "\n",
    "        PINN_point_sampler = gen_samples(BRIDGE.domain, n_sim_samples)\n",
    "        GINN_point_sampler = gen_samples(BRIDGE.domain, n_opt_samples)\n",
    "\n",
    "        # ===================== First optimization stage =====================\n",
    "        print(\"=== Generating Geometry: ===\")\n",
    "        t_ginn0 = time.perf_counter()\n",
    "\n",
    "        for it in tqdm(range(self.n_pre_training_iterations_GINN), desc=\"GINN Training\"):\n",
    "            self.shape_generation_step(SDF_GINN_optimizer, epoch=it)\n",
    "\n",
    "            if (it % 40) == 0:\n",
    "                fig = plot_GINN_geometry(self.test_case, 80000, self.SDF_GINN_model, 0.5)\n",
    "                fig.savefig(os.path.join(self.save_path, f\"ginn-geometry-{it:06d}.png\"), dpi=150)\n",
    "                plt.close(fig)\n",
    "\n",
    "                geometry_losses = GINN_losses(\n",
    "                    self.SDF_GINN_model,\n",
    "                    self.test_case,\n",
    "                    self.GINN_hparams,\n",
    "                    self.PH,\n",
    "                    self.boundary_sampler,\n",
    "                    self.enforce_density\n",
    "                )\n",
    "\n",
    "                coords = next(self.point_sampler).to(self.device)\n",
    "                eik_loss = geometry_losses.eikonal_loss(coords)\n",
    "                env_loss = geometry_losses.design_envelope_loss()\n",
    "                connect_loss = geometry_losses.connectivity_loss()\n",
    "                hole_loss = geometry_losses.holes_loss()\n",
    "                int_loss = geometry_losses.interface_loss()\n",
    "                norm_loss = geometry_losses.surface_normal_loss()\n",
    "                thick_loss = geometry_losses.prescribed_thickness_loss(coords)\n",
    "                prohib_loss = geometry_losses.prohibited_region_loss(coords)\n",
    "                smooth_loss = geometry_losses.smoothness_loss(it)\n",
    "\n",
    "                smooth_loss_scaled = self.loss_ramp_up(\n",
    "                    it,\n",
    "                    self.GINN_hparams['curv_start_epoch'],\n",
    "                    self.GINN_hparams['curv_ramp_epochs']\n",
    "                ) * smooth_loss\n",
    "\n",
    "                total_loss = (\n",
    "                    eik_loss + env_loss + connect_loss + hole_loss + int_loss +\n",
    "                    norm_loss + thick_loss + prohib_loss + smooth_loss_scaled\n",
    "                )\n",
    "\n",
    "                print(f\"Eikonal Loss: {eik_loss.item():.6f}\")\n",
    "                print(f\"Envelope Loss: {env_loss.item():.6f}\")\n",
    "                print(f\"Connectivity Loss: {connect_loss.item():.6f}\")\n",
    "                print(f\"Holes Loss: {hole_loss.item():.6f}\")\n",
    "                print(f\"Interface Loss: {int_loss.item():.6f}\")\n",
    "                print(f\"Surface Normal Loss: {norm_loss.item():.6f}\")\n",
    "                print(f\"Prescribed Thickness Loss: {thick_loss.item():.6f}\")\n",
    "                print(f\"Prohibited Region Loss: {prohib_loss.item():.6f}\")\n",
    "                print(f\"Smoothness Loss: {smooth_loss_scaled.item():.6f}\")\n",
    "                print(f\"Total Loss: {total_loss.item():.6f}\")\n",
    "                print(' ')\n",
    "                print(' ')\n",
    "\n",
    "                self.ginn_hist_iters.append(it)\n",
    "                self.ginn_hist_eik.append(eik_loss.item())\n",
    "                self.ginn_hist_env.append(env_loss.item())\n",
    "                self.ginn_hist_connect.append(connect_loss.item())\n",
    "                self.ginn_hist_hole.append(hole_loss.item())\n",
    "                self.ginn_hist_int.append(int_loss.item())\n",
    "                self.ginn_hist_norm.append(norm_loss.item())\n",
    "                self.ginn_hist_thick.append(thick_loss.item())\n",
    "                self.ginn_hist_prohib.append(prohib_loss.item())\n",
    "                self.ginn_hist_smooth.append(smooth_loss_scaled.item())\n",
    "                self.ginn_hist_total.append(total_loss.item())\n",
    "\n",
    "                # CSV log for GINN losses\n",
    "                self._log_ginn_losses_csv(\n",
    "                    it,\n",
    "                    eik_loss.item(),\n",
    "                    env_loss.item(),\n",
    "                    connect_loss.item(),\n",
    "                    hole_loss.item(),\n",
    "                    int_loss.item(),\n",
    "                    norm_loss.item(),\n",
    "                    thick_loss.item(),\n",
    "                    prohib_loss.item(),\n",
    "                    smooth_loss_scaled.item(),\n",
    "                    total_loss.item()\n",
    "                )\n",
    "\n",
    "                self._save_ginn_training_curves(it)\n",
    "\n",
    "                # periodic checkpoint during GINN pretrain\n",
    "                if (it % self.model_save_interval) == 0:\n",
    "                    self._save_models(tag=f\"ginn-{it:06d}\")\n",
    "                self._maybe_log_eval_metrics(phase=\"GINN\", it=it, model_kind=\"sdf\")\n",
    "\n",
    "\n",
    "        self.time_ginn_pretrain = time.perf_counter() - t_ginn0\n",
    "        self._log_timing(\"ginn_pretrain_total\", self.time_ginn_pretrain)\n",
    "\n",
    "        # Initialize volume ratio from SDF\n",
    "        grid_xy = get_grid_centers(BRIDGE.domain, n_opt_samples).astype(np.float32)\n",
    "        xt = torch.tensor(grid_xy, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            sdf_field = self.SDF_GINN_model(xt).squeeze()\n",
    "            occ = torch.sigmoid(-1000 * sdf_field).clamp(0, 1)\n",
    "            init_vol_frac = float(occ.mean().item())\n",
    "\n",
    "        start_ratio = self.volume_ratio if self.volume_ratio is not None else init_vol_frac\n",
    "        self.volume_ratio = self._safe_clip_ratio(\n",
    "            start_ratio, self.vol_frac_min, self.vol_frac_max\n",
    "        )\n",
    "        self.density_GINN_model.volume_ratio = float(self.volume_ratio)\n",
    "        target_volume = float(self.volume_ratio) * float(BRIDGE.domain_volume)\n",
    "\n",
    "        # ===================== PINN pre-training =====================\n",
    "        print(\"=== Pre-Training PINN: ===\")\n",
    "        for _ in tqdm(range(self.n_pre_training_iterations_PINN), desc=\"PINN Pre-Training\"):\n",
    "            coords = next(PINN_point_sampler)\n",
    "            self.PINN_update_step(coords, PINN_optimizer, n_opt_samples)\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "        # ===================== density GINN pre-training =====================\n",
    "        print(\"=== Pre-Training density GINN: ===\")\n",
    "        for _ in tqdm(range(self.n_pre_training_iterations_PINN), desc=\"GINN Training\"):\n",
    "            coords = next(GINN_point_sampler)\n",
    "            self.density_PINN_initialization_step(coords, density_GINN_optimizer)\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "        # ---- Initial saves ---------------------------\n",
    "        rho_img0, _ = predict_densities_image_2d(\n",
    "            self.density_GINN_model, BRIDGE.domain, n_opt_samples, self.enforce_density\n",
    "        )\n",
    "        save_densities_to_file(\n",
    "            rho_img0,\n",
    "            os.path.join(self.save_path, f\"density-{'%06d' % 0}.png\"),\n",
    "            BRIDGE.domain\n",
    "        )\n",
    "        print('saving initial geometry image to', self.save_path)\n",
    "\n",
    "        u0, v0, s0, r0 = predict_uv_sigma_image_2d_binary(\n",
    "            self.u_model,\n",
    "            self.v_model,\n",
    "            self.density_GINN_model,\n",
    "            BRIDGE.domain,\n",
    "            test_case=self.test_case,\n",
    "            rho_threshold_plot=self.plot_threshold\n",
    "        )\n",
    "        save_uv_sigma_to_file(\n",
    "            u0, v0, s0, r0,\n",
    "            os.path.join(self.save_path, f\"uvsigma-{'%06d' % 0}.png\"),\n",
    "            BRIDGE.domain,\n",
    "            test_case = self.test_case,\n",
    "            rho_threshold=self.plot_threshold\n",
    "        )\n",
    "\n",
    "        save_density_scatter_iso_to_file(\n",
    "            self.density_GINN_model,\n",
    "            BRIDGE.domain,\n",
    "            os.path.join(self.save_path, f\"iso-{'%06d' % 0}.png\"),\n",
    "            grid_resolution=600,\n",
    "            iso=self.plot_threshold\n",
    "        )\n",
    "\n",
    "        sigma0, sigma0_max, vol0 = self._measure_stress_volume_2d()\n",
    "        comp0 = self._estimate_compliance_internal_energy(n_opt_samples)\n",
    "        self.hist_iters.append(0)\n",
    "        self.hist_sigma_metric.append(sigma0)\n",
    "        self.hist_volume.append(vol0)\n",
    "        self.hist_compliance.append(comp0)\n",
    "        self._save_training_curves(0)\n",
    "        self._save_stress_histogram(0, s0, r0, sigma0, sigma0_max)\n",
    "        self._log_optimization_metrics_csv(0, sigma0, sigma0_max, vol0, comp0)\n",
    "        self._maybe_log_eval_metrics(phase=\"TOPO\", it=0, model_kind=\"density\")\n",
    "        self._save_models(tag=\"init\")\n",
    "\n",
    "        # ===================== MAIN OPT LOOP =====================\n",
    "        t_topo_phase0 = time.perf_counter()\n",
    "\n",
    "        for it in range(1, self.n_opt_iterations + 1):\n",
    "            print(f\"\\n=== Optimization iteration {it}/{self.n_opt_iterations} ===\")\n",
    "\n",
    "            # ---- Update PINN ----\n",
    "            for _ in tqdm(range(self.n_sim_iterations), desc=\"Updating PINN\"):\n",
    "                coords = next(PINN_point_sampler)\n",
    "                self.PINN_update_step(coords, PINN_optimizer, n_opt_samples)\n",
    "\n",
    "            # ---- Build OC targets ----\n",
    "            old_densities_list = []\n",
    "            sensitivities_list = []\n",
    "            coords_list = []\n",
    "\n",
    "            topo_funcs = topology_optimization(\n",
    "                self.u_model,\n",
    "                self.v_model,\n",
    "                self.density_GINN_model,\n",
    "                n_opt_samples,\n",
    "                self.training_hparams,\n",
    "                self.enforce_density\n",
    "            )\n",
    "\n",
    "            for _ in tqdm(range(self.n_opt_batches), desc=\"Computing Target Densities\"):\n",
    "                coords = next(GINN_point_sampler)\n",
    "                densities, sensitivities = topo_funcs.compute_sensitivities(coords)\n",
    "\n",
    "                densities, sensitivities = self.enforce_density.apply(\n",
    "                    coords,\n",
    "                    densities,\n",
    "                    sensitivities,\n",
    "                    n_opt_samples,\n",
    "                    BRIDGE.domain\n",
    "                )\n",
    "\n",
    "                filtered_sensitivities = topo_funcs.apply_sensitivity_filter(\n",
    "                    coords,\n",
    "                    densities,\n",
    "                    sensitivities,\n",
    "                    n_samples=n_opt_samples,\n",
    "                    domain=BRIDGE.domain,\n",
    "                    radius=self.filter_radius\n",
    "                )\n",
    "\n",
    "                old_densities_list.append(densities)\n",
    "                sensitivities_list.append(filtered_sensitivities)\n",
    "                coords_list.append(coords)\n",
    "\n",
    "            target_density_list = topo_funcs.compute_target_densities(\n",
    "                old_densities_list,\n",
    "                sensitivities_list,\n",
    "                sample_volume=BRIDGE.domain_volume,\n",
    "                target_volume=target_volume,\n",
    "                max_move=0.2,\n",
    "                damping_parameter=0.5\n",
    "            )\n",
    "\n",
    "            # ---- Apply density update pass ----\n",
    "            for coords, target_density in tqdm(\n",
    "                    list(zip(coords_list, target_density_list)),\n",
    "                    desc=\"Optimizing density field\",\n",
    "                    total=len(coords_list)):\n",
    "                self.topology_optimization_step(\n",
    "                    density_GINN_optimizer,\n",
    "                    coords,\n",
    "                    target_density\n",
    "                )\n",
    "\n",
    "            # ---- Stress & volume on binary geometry ----\n",
    "            sigma_metric, sigma_max, current_volume = self._measure_stress_volume_2d()\n",
    "\n",
    "\n",
    "            # --- EMA to fight oscillations ---\n",
    "            if self.sigma_ema is None:\n",
    "                self.sigma_ema = sigma_metric\n",
    "            else:\n",
    "                a = self.sigma_ema_alpha\n",
    "                self.sigma_ema = (1.0 - a) * self.sigma_ema + a * sigma_metric\n",
    "\n",
    "            u_img, v_img, sigma_img, rho_img = predict_uv_sigma_image_2d_binary(\n",
    "                self.u_model,\n",
    "                self.v_model,\n",
    "                self.density_GINN_model,\n",
    "                BRIDGE.domain,\n",
    "                test_case=self.test_case,\n",
    "                rho_threshold_plot=self.plot_threshold\n",
    "            )\n",
    "\n",
    "            # ---- Compliance on current SIMP design ----\n",
    "            comp_now = self._estimate_compliance_internal_energy(n_opt_samples)\n",
    "\n",
    "            # ---- Log history ----\n",
    "            self.hist_iters.append(it)\n",
    "            self.hist_sigma_metric.append(sigma_metric)\n",
    "            self.hist_volume.append(current_volume)\n",
    "            self.hist_compliance.append(comp_now)\n",
    "\n",
    "            # ---- CSV metrics log every iteration ----\n",
    "            self._log_optimization_metrics_csv(\n",
    "                it, sigma_metric, sigma_max, current_volume, comp_now\n",
    "            )\n",
    "\n",
    "            # ---- Periodic curve save ----\n",
    "            if (it % self.save_interval) == 0:\n",
    "                self._save_training_curves(it)\n",
    "\n",
    "            # ---- Volume ratio controller ----\n",
    "            r = self.sigma_ema / (self.sigma_allow + 1e-20)\n",
    "\n",
    "            if r < 1.0 - self.stress_tol:\n",
    "                gap = 1.0 - r\n",
    "                step = float(np.clip(self.alpha_decrease * gap,\n",
    "                                     self.vol_step_min,\n",
    "                                     self.vol_step_max))\n",
    "                new_ratio = float(self.volume_ratio) * (1.0 - step)\n",
    "            elif r > 1.0 + self.stress_tol:\n",
    "                gap = r - 1.0\n",
    "                step = float(np.clip(self.beta_increase * gap,\n",
    "                                     self.vol_step_min,\n",
    "                                     self.vol_step_max))\n",
    "                new_ratio = float(self.volume_ratio) * (1.0 + step)\n",
    "            else:\n",
    "                new_ratio = float(self.volume_ratio)\n",
    "\n",
    "            self.volume_ratio = self._safe_clip_ratio(\n",
    "                new_ratio, self.vol_frac_min, self.vol_frac_max\n",
    "            )\n",
    "            self.density_GINN_model.volume_ratio = float(self.volume_ratio)\n",
    "            target_volume = float(self.volume_ratio) * float(BRIDGE.domain_volume)\n",
    "\n",
    "            print(\n",
    "                f\"[iter {it}] C={comp_now:.4g} | σ_metric={sigma_metric:.4g} (σ_allow={self.sigma_allow:.4g}) \"\n",
    "                f\"| σ_max={sigma_max:.4g} | V={current_volume:.6g} \"\n",
    "                f\"| vol_ratio→{self.volume_ratio:.4f} \"\n",
    "                f\"(target={target_volume/BRIDGE.domain_volume:.4f}·V_domain)\"\n",
    "            )\n",
    "\n",
    "            # ---- Periodic saves ----\n",
    "            if (it % self.save_interval) == 0:\n",
    "                print('Saving images to', self.save_path)\n",
    "                self._maybe_log_eval_metrics(phase=\"TOPO\", it=it, model_kind=\"density\")\n",
    "\n",
    "                save_density_scatter_iso_to_file(\n",
    "                    self.density_GINN_model,\n",
    "                    BRIDGE.domain,\n",
    "                    os.path.join(self.save_path, f\"iso-{it:06d}.png\"),\n",
    "                    grid_resolution=600,\n",
    "                    iso=self.plot_threshold\n",
    "                )\n",
    "\n",
    "                rho_img_save, _ = predict_densities_image_2d(\n",
    "                    self.density_GINN_model,\n",
    "                    BRIDGE.domain,\n",
    "                    n_opt_samples,\n",
    "                    self.enforce_density\n",
    "                )\n",
    "                save_densities_to_file(\n",
    "                    rho_img_save,\n",
    "                    os.path.join(self.save_path, f\"density-{it:06d}.png\"),\n",
    "                    BRIDGE.domain\n",
    "                )\n",
    "\n",
    "                save_uv_sigma_to_file(\n",
    "                    u_img, v_img, sigma_img, rho_img,\n",
    "                    os.path.join(self.save_path, f\"uvsigma-{it:06d}.png\"),\n",
    "                    BRIDGE.domain,\n",
    "                    test_case = self.test_case,\n",
    "                    rho_threshold=self.plot_threshold\n",
    "                )\n",
    "\n",
    "                self._save_stress_histogram(it,\n",
    "                                            sigma_img,\n",
    "                                            rho_img,\n",
    "                                            sigma_metric,\n",
    "                                            sigma_max)\n",
    "\n",
    "                self._save_topology_density_curves(it)\n",
    "\n",
    "                self._log_optimization_losses_csv(\n",
    "                    it, comp_now, sigma_metric, sigma_max, current_volume\n",
    "                )\n",
    "\n",
    "                # ---- Model checkpoint ----\n",
    "                if (it % self.model_save_interval) == 0:\n",
    "                    self._save_models(tag=f\"opt-{it:06d}\")\n",
    "\n",
    "        self.time_topology_phase = time.perf_counter() - t_topo_phase0\n",
    "        self._log_timing(\"topology_phase_total\", self.time_topology_phase)\n",
    "        self._log_timing(\"density_updates_total\", self.time_density_updates)\n",
    "\n",
    "        # ---- Final density & iso ----\n",
    "        rho_img, _ = predict_densities_image_2d(\n",
    "            self.density_GINN_model, BRIDGE.domain, n_opt_samples, self.enforce_density\n",
    "        )\n",
    "        save_densities_to_file(\n",
    "            rho_img,\n",
    "            os.path.join(self.save_path, f\"density-final.png\"),\n",
    "            BRIDGE.domain\n",
    "        )\n",
    "\n",
    "        save_density_scatter_iso_to_file(\n",
    "            self.density_GINN_model,\n",
    "            BRIDGE.domain,\n",
    "            os.path.join(self.save_path, f\"iso-final.png\"),\n",
    "            grid_resolution=600,\n",
    "            iso=self.plot_threshold\n",
    "        )\n",
    "\n",
    "        # Final curves snapshot\n",
    "        if len(self.hist_iters) > 0:\n",
    "            self._save_training_curves(self.hist_iters[-1])\n",
    "\n",
    "        # ---- Final total time ----\n",
    "        self.time_total = time.perf_counter() - t_total0\n",
    "        self._log_timing(\n",
    "            \"final_total\",\n",
    "            self.time_total,\n",
    "            extra={\n",
    "                \"ginn_pretrain\": self.time_ginn_pretrain,\n",
    "                \"topology_phase\": self.time_topology_phase,\n",
    "                \"density_updates\": self.time_density_updates\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Final checkpoint\n",
    "        self._save_models(tag=\"final\")\n",
    "        self._maybe_log_eval_metrics(phase=\"TOPO\", it=int(self.n_opt_iterations), model_kind=\"density\")\n",
    "\n",
    "        print(\"Done. Results in:\", self.save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306bfdc",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770668091536,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "e306bfdc"
   },
   "outputs": [],
   "source": [
    "hparams_model = {\n",
    "    'Model_type'         : 'SIREN',\n",
    "    'num_hidden_layers'  : 3,\n",
    "    'num_hidden_neurons' : 280,\n",
    "\n",
    "    'SIREN_hparams':\n",
    "                    {\n",
    "                        'Model_type' : 'SIREN',\n",
    "                        'layers'     : [180,180,180,180],\n",
    "                        'dimensionality': 2,\n",
    "                        'w0_initial' : 1,\n",
    "                        'w0'         : 1,\n",
    "                        'skip_connection' : True,\n",
    "                    },\n",
    "    'GINN_SIREN_hparams':\n",
    "                    {\n",
    "                        'Model_type' : 'SIREN',\n",
    "                        'layers'     : [180,180,180,180],\n",
    "                        'dimensionality': 2,\n",
    "                        'w0_initial' : 15,\n",
    "                        'w0'         : 2,\n",
    "                        'skip_connection' : True,\n",
    "                    },\n",
    "    'WIRE_hparams':\n",
    "                    {   'Model_type' : 'WIRE',\n",
    "                        'layers'     : [80,80,80,80],\n",
    "                        'dimensionality': 2,\n",
    "                        'w0_initial' : 30,\n",
    "                        'w0'         : 10,\n",
    "                        'sigma0'     : 10,\n",
    "                        'sigma0_initial' : 10,\n",
    "                        'layer_type': 'real_gabor',\n",
    "                        'trainable' : False,\n",
    "                        'skip_connection' : True,\n",
    "                    },\n",
    "    'MLP_hparams':\n",
    "                    {\n",
    "                        'Model_type' : 'MLP',\n",
    "                        'layers'     : [180,180,180,180],\n",
    "                        'dimensionality': 2,\n",
    "                        'activation_function' : 'softplus',\n",
    "                        'use_bias'        : True,\n",
    "                        'use_batch_norm'  : False,\n",
    "                        'use_dropout'     : False,\n",
    "                        'dropout_rate'    : 0.1,\n",
    "                        'skip_connection' : True,\n",
    "                    },\n",
    "}\n",
    "\n",
    "hparams_feature_expansion = {\n",
    "    'Feature Type'      : 'None',\n",
    "    'Num Frequencies'   : 3,\n",
    "    'Max Frequency'     : 100,\n",
    "}\n",
    "\n",
    "density_constraint_hparams= {\n",
    "'enabled': True,\n",
    "'priority':\"keep_over_prohibit\",\n",
    "}\n",
    "\n",
    "topo_hparams = {\n",
    "    'save_path':     \"./B_DELETE\",\n",
    "    'volume_ratio':  0.5,\n",
    "    'lr_PINN':       3e-4,\n",
    "    'lr_GINN':       3e-4,\n",
    "    'save_interval': 5,\n",
    "    'filter_radius': 2.0,\n",
    "    'n_opt_iterations': 200,\n",
    "    'n_sim_iterations': 1000,\n",
    "    'n_pre_training_iterations_PINN': 2500,\n",
    "    'n_pre_training_iterations_GINN': 500, #800\n",
    "    'n_opt_batches': 50,\n",
    "    'seed':          43,\n",
    "    'rho_treshold':       0.4,\n",
    "    'sigma_allow':        2000.0,\n",
    "    'stress_metric':     'percentile', #KS or percentile\n",
    "    'stress_percentile':  0.995,\n",
    "    'rho_mask_threshold': 0.6,\n",
    "    'stress_tol':         0.02,\n",
    "    'ks_rho': 50.0,\n",
    "    'vol_frac_min':      0.02,\n",
    "    'vol_frac_max':      0.98,\n",
    "    'vol_step_min':      0.005,\n",
    "    'vol_step_max':      0.05,\n",
    "    'alpha_decrease':    0.50,\n",
    "    'beta_increase':     0.25,\n",
    "}\n",
    "\n",
    "training_hparams = {\n",
    "    'total_sample_points': 10000,\n",
    "    'batch_size': 5000,\n",
    "    'num_neumann_points': 10000,\n",
    "    'dirichlet_pts': 5000,\n",
    "    'mollifier_alpha': 1,\n",
    "    'density_alpha': 2,\n",
    "    'density_exponent': 3,\n",
    "    \"grad_clipping_on\": True,\n",
    "    \"grad_clip\": 0.5,\n",
    "    \"auto_clip_on\": True,\n",
    "    \"auto_clip_percentile\": 0.9,\n",
    "    \"auto_clip_hist_len\": 100,\n",
    "    \"auto_clip_min_len\": 10,\n",
    "    \"refine_study\": False,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "GINN_hparams ={\n",
    "'eikonal_loss_weight' : 0.1,\n",
    "'envelope_loss_weight':1,\n",
    "'connectivity_loss_weight':1,\n",
    "'holes_loss_weight':1,\n",
    "'interface_loss_weight':1,\n",
    "'prescribed_normals_loss_weight':1,\n",
    "'prescribed_thickness_loss_weight':1,\n",
    "'prohibited_region_loss_weight':1,\n",
    "'smoothness_loss_weight': 1e-4,\n",
    "'envelope_loss_weight_density':     1,\n",
    "'connectivity_loss_weight_density': 1,\n",
    "'num_points_envelope_loss':10000,\n",
    "'num_points_connectivity_loss':50000,\n",
    "'envelope_extension_factor': 0.2,\n",
    "'num_points_interface_loss': 4096,\n",
    "'num_points_normals_loss': 4096,\n",
    "'clip_max_value': 1.0e+6,\n",
    "'clip_min_value': 0,\n",
    "'max_curv': 0,\n",
    "'curv_start_epoch': 200,\n",
    "'curv_ramp_epochs':100,\n",
    "}\n",
    "\n",
    "\n",
    "adaptive_weighting_hparams = {\n",
    "        'use_adaptive_weighting': True,\n",
    "        'alpha': 0.90,\n",
    "        'gamma': 1e-2,\n",
    "        'epsilon': 1e-8,\n",
    "        'objective_function': True,\n",
    "        'objective_losses': ['Smoothness Loss'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551a63f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1770668091812,
     "user": {
      "displayName": "Ørjan Jaathun",
      "userId": "11935124029388210670"
     },
     "user_tz": -60
    },
    "id": "d551a63f",
    "outputId": "ff906b0f-a763-4ed0-88fd-b59b1a998630"
   },
   "outputs": [],
   "source": [
    "\n",
    "rho_GINN = density_GINN(hparams_model['SIREN_hparams'],\n",
    "                            hparams_feature_expansion,\n",
    "                            density_alpha=training_hparams['density_alpha'],\n",
    "                            volume_ratio=topo_hparams['volume_ratio'])\n",
    "\n",
    "sdf_GINN = SDF_GINN(hparams_model['GINN_SIREN_hparams'],\n",
    "                    hparams_feature_expansion)\n",
    "\n",
    "u_model = PINN(hparams_model['SIREN_hparams'],\n",
    "               hparams_feature_expansion,\n",
    "               training_hparams['mollifier_alpha'])\n",
    "\n",
    "v_model = PINN(hparams_model['SIREN_hparams'],\n",
    "               hparams_feature_expansion,\n",
    "               training_hparams['mollifier_alpha'])\n",
    "\n",
    "Shape_Generator = model_training(u_model,\n",
    "                                 v_model,\n",
    "                                 rho_GINN,\n",
    "                                 sdf_GINN,\n",
    "                                 training_hparams,\n",
    "                                 topo_hparams,\n",
    "                                 density_constraint_hparams,\n",
    "                                 BRIDGE,\n",
    "                                 adaptive_weighting_hparams,\n",
    "                                 GINN_hparams,\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c41ce0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4c41ce0",
    "outputId": "b513f5b2-7886-495e-d037-ee64a6e0e539"
   },
   "outputs": [],
   "source": [
    "Shape_Generator.generate_geometry()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
